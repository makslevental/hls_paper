Certain areas of science perform experiments that achieve extremely high data rates; 
well-known examples in particle physics are the Compact Muon Solenoid (CMS) and Toroidal Apparatus (ATLAS) experiments at the Large Hadron Collider (LHC).
These experiments observe new collision events every 25~ns (i.e., at a rate of 40~MHz) and for which \emph{triggers} must report detection of ``interesting'' events within, at most, 10~\textmu s ~\cite{pmlr-v42-glig14}.
Similarly, X-ray crystallography employs high-energy diffraction microscopy (HEDM) techniques, which can sample at up to 1~MHz~\cite{doi:10.1063/5.0006531} (and also must trigger within 10~\textmu s).
Such high data rate experiments face the challenge of either processing the data in situ (colocated with the scientific apparatus) or buffering/caching for later retrieval and post-processing.
For example, if CMS and ATLAS were to capture all collision events, they would produce approximately 40 terabytes per second~\cite{BORK2021100619}.
Thus, any improvement in the real-time, near-sensor, processing capabilities of experiment infrastructure can dramatically reduce the burden on downstream infrastructure and compute, in addition to accelerating the pace of experiment design and scientific discovery.

% \ian{I found the next text hard to follow because it doesn't really explain WHY DNNs are being considered in the first place, which is that due to their ability to perform specialized data processing tasks with high efficiency, appropriately trained DNNs are being used to implement analyses that could not otherwise be performed online, at least not without custom silicon.
% Maybe say that first (using BraggNN example to illustrate the large speedups possible: give number), and then explain that particularly at very high data rates, even DNNs cannot deliver required performance on ``conventional'' hardware (GPUs or other such DNN accelerators), which furthermore have large form factors that preclude colocation with sensors.}

In general, classical, physics-based, methods for these data processing tasks cannot meet requisite latency constraints~\cite{Sharma:rw5009}.
Deep neural networks, functioning as learned but efficient approximations of the physics-based methods, and effective in many other academic and commercial domains, have recently been considered for these use cases~\cite{Guest:2018yhq}.
For example, BraggNN~\cite{Liu:fs5198}, a DNN aimed at efficiently identifying Bragg diffraction peaks, achieves detection rates 200x in excess of classical pseudo-Voigt methods, with high accuracy.
Still, as of yet DNN models have not seen wide adoption in these high data-rate experimental domains.
This is due to the limitations imposed by the hardware platforms on which they can typically be deployed: commodity general purpose processors (CPUs), graphics processing units (GPUs) and other, exotic, DNN accelerators.
Primarily, such accelerators cannot meet the hard real-time latency constraints, and secondarily they cannot be easily colocated with complex sensing apparatuses.
Case in point: BraggNN, despite having been shown to have high speedup over the classical pseudo-Voigt peak fitting methods, making determinations in approximately 700\textmu s, still falls far short of the 1\textmu s target for handling the 1 MHz sampling rates necessary of HEDM experiments.
In addition, the current implementation of BraggNN, deployed to either a datacenter class GPU such as a NVIDIA V100, or even a workstation class GPU such as a NVIDIA RTX 2080Ti, has no practicable means to being deployed at the edge, i.e., adjacent or proximal to the high energy microscopy equipment.

This work considers a potential alternative deployment platform for deploying BraggNN, namely Field Programmable Gate Arrays (FPGAs), for the purpose of enabling edge Bragg peak detection in data collected in the course of HEDM.
FPGA represent a fabric of hardware units, such as multiplexers (MUXs), lookup tables (LUTs), block RAMs (BRAMs), flip-flops (FFs), and digital signal processors (DSPs), connected together by programmable interconnects.
FPGAs present an appealing alternative data processing platform for low latency scientific use cases for three reasons:
\begin{enumerate}
	\item FPGAs can be configured to implement arbitrary functions with minimal \emph{abstraction cost};
	\item FPGAs can be reconfigured an arbitrary number of times;
	\item FPGAs can be packaged in such a way that they can be colocated with scientific apparatus.
\end{enumerate}
By abstraction cost we mean two aspects of conventional hardware and software:
\begin{enumerate}
	\item the runtime cost of software infrastructure, such as process scheduling and memory management, that enables multitenancy in general purpose compute systems;
	\item the performance and efficiency costs incurred due to compute architecture that aims to be pareto optimal for a wide range of compute tasks; for example, branch prediction circuits in CPUs and fixed width\footnote{Here we refer to the availability of a fixed set of IEEE 754 floating point precisions (usually only single, 32 bit, and double, 64 bit, precision), rather than fixed precision.} floating point units in GPUs.
\end{enumerate}
To wit: the LHC currently employs FPGAs (in combination with application specific integrated circuits) for their Level-1 triggering system.

But FPGAs are not a free lunch; there are two principal challenges in deploying DNNs to FPGAs.
The first is in translating existing representations of DNNs to Register Transfer Level (RTL) representations, which can be used to configure FPGAs.
Note, DNNs are typically represented in very high-level deep learning frameworks (such as PyTorch and Tensorflow) that abstract away all implementation details. 
For example, a convolution operation, specified as \texttt{torch.nn.Conv2d(1, 16, 3)}, indicates (prima facie) neither the precise data path of the inputs nor the control flow of the executor; high-level DNN frameworks aim to be \emph{declarative}, intentionally eschewing such low-level details.
Thus it becomes necessary to \emph{lower} the high-level representation to a lower level representation, which explicitly represents the data path and control flow; effectively one must compile the DNN.
But compilation necessarily implies some instruction set architecture, i.e. some basic set of data path (e.g., \texttt{mov}, \texttt{push}) and control flow (e.g., \texttt{jmp}, \texttt{jne}) primitives,
whereas FPGAs support no such primitives.
Indeed, for typical use-cases of FPGAs, the development methodology involves a great deal of hand-written or ``hand-generated''~\cite{nikhil2004bluespec} design of the primitive components (adders, multipliers, buses, etc.), a methodology dramatically distinct from conventional software design in general, and DNN design in particular.
Thus, deploying to FPGA entails reimagining a DNN model as compute architecture unto itself, including sophisticated considerations such as operation scheduling, register pipelining, and wire delay.
Recently, with the advent of DNN compiler technologies, such as Multi-level Intermediate Representation (MLIR)~\cite{https://doi.org/10.48550/arxiv.2002.11054}, and supported by advanced High-Level Synthesis (HLS) tools, it has become possible to produce RTL representations of DNN models, with minimal intervention on the part of the user.
This work explores such a design methodology, including some of the currently available tools that aim to support an ``end-to-end flow'', i.e., a design process that takes as input a high-level representation of a DNN and produces synthesizable RTL.

The second principal challenge to deploying DNNs to FPGAs are more fundamental; current generation FPGAs \maxx{TODO: <cannot do what? cannot take the heat? cannot route?>}.
This work investigates techniques for mitigating issues related to these hardware limitations, including approximations that reduce circuit complexity, and alternative scheduling methods. 

In summary we deploy BraggNN to FPGA by performing a series of progressive lowerings, starting from a high-level representation (a PyTorch model) and culminating in synthesizable RTL (i.e., a representation that can be directly mapped to FPGA hardware).
We show that under certain assumptions, our approach produces inference latencies lower than that of any existing tool, achieving a peak end-to-end latency of 3 µs/333 KHz.
This latency represents a 200x improvement over the GPU implementation of BraggNN and is only a factor of three distant from the ultimate 1 µs/1 MHz latency target.
Our work includes a survey of existing general purpose tools aimed at performing such lowerings, as well as our own novel approach.
Thus, the primary contributions of this paper are:
\begin{enumerate}
	\item A comprehensive discussion of MLIR, as state-of-the-art DNN compiler technology, and its relevance to FPGA design;
	\item A description of generally applicable techniques for translating a DNN model from a high-level representation to a RTL representation;
	\item An application of the aforementioned techniques to the case of BraggNN.
\end{enumerate}
The remainder of this article is structured as such:
\begin{enumerate}
	\item Background on DNN compiler technology, high-level synthesis, and FPGA implementation;
	\item Our approach as compared to existing tools, with particular on difficulties faced (and overcome);
	\item An evaluation of our approach, as compared to existing tools, in terms of the latency and resource usage of BraggNN;
	\item High priority goals, as we see them, for making this flow more ergonomic.
\end{enumerate}