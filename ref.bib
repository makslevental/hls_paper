@misc{memento:rfc,
	author = {Herbert {Van de Sompel} and
			Michael L. Nelson and
			Robert Sanderson},
	title = {{HTTP Framework for Time-Based Access to Resource States -- Memento, Internet RFC 7089}},
	year = {2013},
	howpublished = {\url{https://tools.ietf.org/html/rfc7089}},
}


@InProceedings{pmlr-v42-glig14,
	title = {Real-time data analysis at the {LHC}: present and future},
	author = {Gligorov, Vladimir},
	booktitle = {NIPS Workshop on High-energy Physics and Machine Learning},
	pages = {1--18},
	year = {2015},
	editor = {Cowan, Glen and Germain, Cecile and Guyon, Isabelle and Kegl, Balazs and Rousseau, David},
	volume = {42},
	seriesX = {Proceedings of Machine Learning Research},
	addressX = {Montreal, Canada},
	publisherX = {PMLR},
	pdfX= {http://proceedings.mlr.press/v42/glig14.pdf},
	urlX = {https://proceedings.mlr.press/v42/glig14.html},
	abstract = {}
}


@article{doi:10.1063/5.0006531,
	author = {Keunecke,Marius and M{\"o}ller,Christina and Schmitt,David and Nolte,Hendrik and Jansen,G. S. Matthijs and Reutzel,Marcel and Gutberlet,Marie and Halasi,Gyula and Steil,Daniel and Steil,Sabine and Mathias,Stefan},
	doi = {10.1063/5.0006531},
	eprint = {https://doi.org/10.1063/5.0006531},
	journal = {Review of Scientific Instruments},
	number = {6},
	pages = {063905},
	title = {Time-resolved momentum microscopy with a 1 MHz high-harmonic extreme ultraviolet beamline},
	url = {https://doi.org/10.1063/5.0006531},
	volume = {91},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1063/5.0006531} }


@article{BORK2021100619,
	abstract = {},
	author = {Rolf Bork and Jonathan Hanks and David Barker and Joseph Betzwieser and Jameson Rollins and Keith Thorne and Erik {von Reis}},
	doi = {https://doi.org/10.1016/j.softx.2020.100619},
	issn = {2352-7110},
	journal = {SoftwareX},
	keywords = {Real-time processing, Feedback control, Hardware control, Data acquisition},
	pages = {100619},
	title = {advligorts: The Advanced LIGO real-time digital control and data acquisition system},
	url = {https://www.sciencedirect.com/science/article/pii/S2352711020303320},
	volume = {13},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S2352711020303320},
	bdsk-url-2 = {https://doi.org/10.1016/j.softx.2020.100619} }

@article{Guest:2018yhq,
author = "Guest, Dan and Cranmer, Kyle and Whiteson, Daniel",
title = "Deep Learning and its Application to {LHC} Physics",
eprint = "1806.11484",
archivePrefix = "arXiv",
primaryClass = "hep-ex",
doi = "10.1146/annurev-nucl-101917-021019",
journal = "Ann. Rev. Nucl. Part. Sci.",
volume = "68",
pages = "161--181",
year = "2018"
}

@inproceedings{paszke2017automatic,
	title = {Automatic differentiation in {PyTorch}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	booktitle={31st Conference on Neural Information Processing Systems},
	year = {2017}
}

@misc{https://doi.org/10.48550/arxiv.2002.11054,
	doi = {10.48550/ARXIV.2002.11054},

	url = {https://arxiv.org/abs/2002.11054},

	author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},

	keywords = {Programming Languages (cs.PL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

	title = {MLIR: A compiler infrastructure for the end of Moore's Law},

	publisher = {arXiv},

	year = {2020},

	copyright = {Creative Commons Attribution 4.0 International}
}


@misc{torch-mlir,
	author = {Sean Silva and Anush Elangovan},
	title = {{Torch-MLIR}},
	year = {2021},
	howpublished = {\url{https://mlir.llvm.org/OpenMeetings/2021-10-07-The-Torch-MLIR-project.pdf}},
}

@misc{polyhedral-mlir,
	author = {Uday Bondhugula},
	title = {Polyhedral compilation opportunities in MLIR},
	year = {2020},
	howpublished = {\url{https://acohen.gitlabpages.inria.fr/impact/impact2020/slides/IMPACT_2020_keynote.pdf}},
}


@incollection{Zhang2008,
	abstract = {The rapid increase of complexity in System-on-a-Chip design urges the design community to raise the level of abstraction beyond RTL. Automated behavior-level and system-level synthesis are naturally identified as next steps to replace RTL synthesis and will greatly boost the adoption of electronic system-level (ESL) design. High-level executable specifications, such as C, C++, or SystemC, are also preferred for system-level verification and hardware/software co-design.},
	address = {Dordrecht},
	author = {Zhang, Zhiru and Fan, Yiping and Jiang, Wei and Han, Guoling and Yang, Changqi and Cong, Jason},
	booktitle = {High-Level Synthesis},
	doiX = {10.1007/978-1-4020-8588-8_6},
	editorX = {Coussy, Philippe and Morawiec, Adam},
	isbn = {978-1-4020-8588-8},
	pages = {99--112},
	publisher = {Springer Netherlands},
	title = {AutoPilot: A Platform-Based ESL Synthesis System},
	urlX = {https://doi.org/10.1007/978-1-4020-8588-8_6},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1007/978-1-4020-8588-8_6} }

@article{10.1145/2514740,
	author = {Canis, Andrew and Choi, Jongsok and Aldham, Mark and Zhang, Victor and Kammoona, Ahmed and Czajkowski, Tomasz and Brown, Stephen D. and Anderson, Jason H.},
	title = {LegUp: An Open-Source High-Level Synthesis Tool for FPGA-Based Processor/Accelerator Systems},
	year = {2013},
	issue_date = {September 2013},
	volume = {13},
	number = {2},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/2514740},
	doi = {10.1145/2514740},
	abstract = {It is generally accepted that a custom hardware implementation of a set of computations will provide superior speed and energy efficiency relative to a software implementation. However, the cost and difficulty of hardware design is often prohibitive, and consequently, a software approach is used for most applications. In this article, we introduce a new high-level synthesis tool called LegUp that allows software techniques to be used for hardware design. LegUp accepts a standard C program as input and automatically compiles the program to a hybrid architecture containing an FPGA-based MIPS soft processor and custom hardware accelerators that communicate through a standard bus interface. In the hybrid processor/accelerator architecture, program segments that are unsuitable for hardware implementation can execute in software on the processor. LegUp can synthesize most of the C language to hardware, including fixed-sized multidimensional arrays, structs, global variables, and pointer arithmetic. Results show that the tool produces hardware solutions of comparable quality to a commercial high-level synthesis tool. We also give results demonstrating the ability of the tool to explore the hardware/software codesign space by varying the amount of a program that runs in software versus hardware. LegUp, along with a set of benchmark C programs, is open source and freely downloadable, providing a powerful platform that can be leveraged for new research on a wide range of high-level synthesis topics.},
	journal = {ACM Trans. Embed. Comput. Syst.},
	articleno = {24},
	keywords = {power, performance, field-programmable gate arrays, FPGAs, synthesis, hardware/software codesign, High-level synthesis}
}

@INPROCEEDINGS{ferrandi2021bambu,

	author = {Ferrandi, Fabrizio and Castellana, Vito Giovanni

			and Curzel, Serena and Fezzardi, Pietro and Fiorito, Michele

			and Lattuada, Marco and Minutoli, Marco and Pilato, Christian

			and Tumeo, Antonino},

	booktitle = {58th ACM/IEEE Design Automation Conference},

	title = {Bambu: an Open-Source Research Framework for the

			High-Level Synthesis of Complex Applications},

	year = {2021},

	pages = {1327-1330},

	publisher = {{IEEE}},

}

@phdthesis{tuprints9272,
	title = {Advances in ILP-based Modulo Scheduling for High-Level Synthesis},
	year = {2019},
	xlanguage = {en},
	author = {Julian Oppermann},
	school = {Technische Universit{\"a}t},
	address = {Darmstadt},
	url = {http://tuprints.ulb.tu-darmstadt.de/9272/},
	abstract = {In today's heterogenous computing world, field-programmable gate arrays (FPGA) represent the energy-efficient alternative to generic processor cores and graphics accelerators. However, due to their radically different computing model, automatic design methods, such as high-level synthesis (HLS), are needed to harness their full power. HLS raises the abstraction level to behavioural descriptions of algorithms, thus freeing designers from dealing with tedious low-level concerns, and enabling a rapid exploration of different microarchitectures for the same input specification. In an HLS tool, scheduling is the most influential step for the performance of the generated accelerator. Specifically, modulo schedulers enable a pipelined execution, which is a key technique to speed up the computation by extracting more parallelism from the input description. In this thesis, we make a case for the use of integer linear programming (ILP) as a framework for modulo scheduling approaches. First, we argue that ILP-based modulo schedulers are practically usable in the HLS context. Secondly, we show that the ILP framework enables a novel approach for the automatic design of FPGA accelerators. We substantiate the first claim by proposing a new, flexible ILP formulation for the modulo scheduling problem, and evaluate it experimentally with a diverse set of realistic test instances. While solving an ILP may incur an exponential runtime in the worst case, we observe that simple countermeasures, such as setting a time limit, help to contain the practical impact of outlier instances. Furthermore, we present an algorithm to compress problems before the actual scheduling. An HLS-generated microarchitecture is comprised of operators, i.e. single-purpose functional units such as a floating-point multiplier. Usually, the allocation of operators is determined before scheduling, even though both problems are interdependent. To that end, we investigate an extension of the modulo scheduling problem that combines both concerns in a single model. Based on the extension, we present a novel multi-loop scheduling approach capable of finding the fastest microarchitecture that still fits on a given FPGA device - an optimisation problem that current commercial HLS tools cannot solve. This proves our second claim.}
}

@INPROCEEDINGS{1688836,  author = {Cong, J. and Zhiru Zhang},  booktitle = {43rd ACM/IEEE Design Automation Conference},   title = {An efficient and versatile scheduling algorithm based on SDC formulation},   year = {2006},  volume = {},  number = {},  pages = {433-438},  doi = {10.1145/1146909.1147025} }

@INPROCEEDINGS{6546003,  author={Soni, Ritesh Kumar and Steiner, Neil and French, Matthew},  booktitle={IEEE 21st Annual Intl Symposium on Field-Programmable Custom Computing Machines},   title={Open-Source Bitstream Generation},   year={2013},  volume={},  number={},  pages={105-112},  doiX={10.1109/FCCM.2013.45}}

@inproceedings{wolf2013yosys,
  title={Yosys-a free Verilog synthesis suite},
  author={Wolf, Clifford and Glaser, Johann and Kepler, Johannes},
  booktitle={21st Austrian Workshop on Microelectronics},
  year={2013}
}


@article{10.1145/2858788.2688521,
author = {Ashari, Arash and Tatikonda, Shirish and Boehm, Matthias and Reinwald, Berthold and Campbell, Keith and Keenleyside, John and Sadayappan, P.},
title = {On Optimizing Machine Learning Workloads via Kernel Fusion},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858788.2688521},
doi = {10.1145/2858788.2688521},
pages = {173-182},
numpages = {10},
keywords = {Dense, GPU, Machine Learning, Sparse, Fused Kernel}
}


@article{osti_1574050,
title = {Hierarchical Roofline analysis for GPUs: Accelerating performance optimization for the NERSC-9 Perlmutter system},
author = {Yang, Charlene and Kurth, Thorsten and Williams, Samuel},
abstractNote = {},
doi = {10.1002/cpe.5547},
journal = {Concurrency and Computation. Practice and Experience},
number = 20,
volume = 32,
place = {United Kingdom},
year = {2019}
}


@book{thomas1971catalogue,
  title={A catalogue of optimizing transformations},
  author={Thomas J. Watson IBM Research Center. Research Division and Allen, FE and Cocke, J},
  year={1971}
}


@book{sanders2019sequential,
  title={Sequential and Parallel Algorithms and Data Structures},
  author={Sanders, Peter and Mehlhorn, Kurt and Dietzfelbinger, Martin and Dementiev, Roman},
  year={2019},
  publisher={Springer},
  chapter={9},
  section={9.4},
}

  @misc{ enwiki:1081681080,
    author = "{Wikipedia contributors}",
    title = "Fast inverse square root --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2022",
    howpublished = "\url{https://en.wikipedia.org/wiki/Fast_inverse_square_root}",
    note = "[Online; accessed 3-May-2022]"
  }

@inproceedings{10.1007/978-0-387-72258-0_14,
	address = {Boston, MA},
	author = {Middendorf, Lars and M{\"u}hlbauer, Felix and Umlauf, Georg and Bobda, Christophe},
	booktitle = {Embedded System Design: Topics, Techniques and Trends},
	pages = {155--164},
	publisher = {Springer US},
	title = {Embedded Vertex Shader in FPGA},
	year = {2007}}

@INPROCEEDINGS{8877424,  author={de Dinechin, Florent},  booktitle={IEEE 26th Symposium on Computer Arithmetic},   title={Reflections on 10 Years of FloPoCo},   year={2019},  volume={},  number={},  pages={187-189},  
doiX={10.1109/ARITH.2019.00042}}


@inproceedings{yehpca2022scalehls,
  title={ScaleHLS: A New Scalable High-Level Synthesis Framework on Multi-Level Intermediate Representation},
  author={Ye, Hanchen and Hao, Cong and Cheng, Jianyi and Jeong, Hyunmin and Huang, Jack and Neuendorffer, Stephen and Chen, Deming},
  booktitle={IEEE International Symposium on High-Performance Computer Architecture},
  year={2022}
}

@INPROCEEDINGS{9516615,  author={Zhang, Jeff Jun and Bohm Agostini, Nicolas and Song, Shihao and Tan, Cheng and Limaye, Ankur and Amatya, Vinay and Manzano, Joseph and Minutoli, Marco and Castellana, Vito Giovanni and Tumeo, Antonino and Wei, Gu-Yeon and Brooks, David},  booktitle={IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP)},   title={Towards Automatic and Agile AI/ML Accelerator Design with End-to-End Synthesis},   year={2021},  volume={},  number={},  pages={218-225},  doi={10.1109/ASAP52443.2021.00040}}

@inproceedings{nikhil2004bluespec,
  title={Bluespec System Verilog: efficient, correct RTL from high level specifications},
  author={Nikhil, Rishiyur},
  booktitle={Proceedings. Second ACM and IEEE International Conference on Formal Methods and Models for Co-Design, 2004. MEMOCODE'04.},
  pages={69--70},
  year={2004},
  organization={IEEE}
}


@article{reconfigfpga,
	abstract = {Reconfigurable computing is a potential paradigm which has been effectively performing mostly in the developments of devices likely Field Programmable Gate Arrays (FPGAs). This paper illustrates the reconfigurable architecture of FPGA and its types. Most widely used high-speed computation fabrics utilized in reconfigurable computing are FPGAs. This paper demonstrates the architectures used in reconfigurable computing and shows the various advantages of using reconfigurable computing design over conventional Application-Specific Integrated Circuits for achieving high level of performance for a desired application. The survey deals with the architecture of FPGAs and their types in detail. This paper also explains the highlights and challenges of fine-grained and coarse-grained architectures. FPGAs have supported partial reconfiguration over the few years. This survey also includes the partial reconfiguration techniques and the various applications of reconfigurability.},
	author = {Babu, Praveenkumar and Parthasarathy, Eswaran},
	date = {2021/02/01},
	date-added = {2022-05-03 17:40:19 -0500},
	date-modified = {2022-05-03 17:40:19 -0500},
	doi = {10.1007/s40031-020-00508-y},
	id = {Babu2021},
	isbn = {2250-2114},
	journal = {Journal of The Institution of Engineers (India): Series B},
	number = {1},
	pages = {143--156},
	title = {Reconfigurable FPGA Architectures: A Survey and Applications},
	url = {https://doi.org/10.1007/s40031-020-00508-y},
	volume = {102},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s40031-020-00508-y}}

@inproceedings{10.1145/3211346.3211348,
author = {Roesch, Jared and Lyubomirsky, Steven and Weber, Logan and Pollock, Josh and Kirisame, Marisa and Chen, Tianqi and Tatlock, Zachary},
title = {Relay: A New IR for Machine Learning Frameworks},
year = {2018},
isbn = {9781450358347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211346.3211348},
doi = {10.1145/3211346.3211348},
abstract = {Machine learning powers diverse services in industry including search, translation, recommendation systems, and security. The scale and importance of these models require that they be efficient, expressive, and portable across an array of heterogeneous hardware devices. These constraints are often at odds; in order to better accommodate them we propose a new high-level intermediate representation (IR) called Relay. Relay is being designed as a purely-functional, statically-typed language with the goal of balancing efficient compilation, expressiveness, and portability. We discuss the goals of Relay and highlight its important design constraints. Our prototype is part of the open source NNVM compiler framework, which powers Amazon's deep learning framework MxNet.},
booktitle = {Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {58–68},
numpages = {11},
keywords = {intermediate representation, compilers, machine learning, differentiable programming},
location = {Philadelphia, PA, USA},
series = {MAPL 2018}
}

@article{vtr,
author = {Murray, Kevin and Petelin, Oleg and Zhong, Sheng and Wang, Jia and Eldafrawy, Mohamed and Legault, Jean-Philippe and Sha, Eugene and Graham, Aaron and Wu, Jean and Walker, Matthew and Zeng, Hanqing and Patros, Panos and Luu, Jason and Kent, Kenneth and Betz, Vaughn},
year = {2020},
pages = {1-55},
title = {VTR 8: High-performance CAD and Customizable FPGA Architecture Modelling},
volume = {13},
journal = {ACM Transactions on Reconfigurable Technology and Systems},
doi = {10.1145/3388617}
}

@inproceedings{mcmullin2022square,
  title={The Square Kilometre Array project update},
  author={J McMullin, J and Diamond, P and Caiazzo, M and Casson, A and Cheetham, T and Dewdney, P and Laing, R and Lewis, B and Schinckel, A and Stringhetti, L},
  booktitle={Ground-based and Airborne Telescopes IX},
  volume={12182},
  pages={263--271},
  year={2022},
  organization={SPIE}
}

@article{grainge2017square,
  title={Square Kilometre Array: The radio telescope of the XXI century},
  author={Grainge, Keith and Alachkar, Bassem and Amy, Shaun and Barbosa, Domingos and Bommineni, Murali and Boven, Paul and Braddock, Ralph and Davis, John and Diwakar, Praveen and Francis, Vishal},
  journal={Astronomy reports},
  volume={61},
  number={4},
  pages={288--296},
  year={2017},
  publisher={Springer}
}

@article{Hammer_2021,
	doi = {10.1088/1748-0221/16/01/p01025},
	url = {https://doi.org/10.1088%2F1748-0221%2F16%2F01%2Fp01025},
	year = 2021,
	publisher = {{IOP} Publishing},
	volume = {16},
	number = {01},
	pages = {P01025--P01025},
	author = {M. Hammer and K. Yoshii and A. Miceli},
  
	title = {Strategies for on-chip digital data compression for X-ray pixel detectors},
  
	journal = {Journal of Instrumentation}
}

@techreport{LHCB-FIGURE-2020-018,
      title         = "Comparison of particle selection algorithms for the {LHCb}
                       Upgrade",
      author = "LHCb Collaboration",
      year          = "2020",
      numberX = {LHCb-FIGURE-2020-018},
      url           = "https://cds.cern.ch/record/2746789",
}

@article{Gligorov_2013,
	doiX = {10.1088/1748-0221/8/02/p02013},
  
	urlX = {https://doi.org/10.1088%2F1748-0221%2F8%2F02%2Fp02013},
  
	year = 2013,
	publisher = {{IOP} Publishing},
  
	volume = {8},
  
	number = {02},
  
	pagesX = {P02013--P02013},
  
	author = {V V Gligorov and M Williams},
  
	title = {Efficient, reliable and fast high-level triggering using a bonsai boosted decision tree},
  
	journal = {J.\ Instrumentation}
}

@article{aaij2020allen,
  title={Allen: A high-level trigger on GPUs for LHCb},
  author={Aaij, Roel and Albrecht, Johannes and Belous, M and Billoir, P and Boettcher, T and Brea Rodr{\'\i}guez, A and Vom Bruch, D and C{\'a}mpora P{\'e}rez, DH and Casais Vidal, A and Craik, DC},
  journal={Computing and Software for Big Science},
  volume={4},
  number={1},
  pages={1--11},
  year={2020},
  publisher={Springer}
}

@article{alzubaidi2021review,
  title={Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions},
  author={Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J and Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and Santamar{\'\i}a, Jos{\'e} and Fadhel, Mohammed A and Al-Amidie, Muthana and Farhan, Laith},
  journal={Journal of Big Data},
  volume={8},
  number={1},
  pages={1--74},
  year={2021},
  publisher={Springer}
}

@misc{https://doi.org/10.48550/arxiv.1603.04467,
  doi = {10.48550/ARXIV.1603.04467},
  
  url = {https://arxiv.org/abs/1603.04467},
  
  author = {Abadi, Mart\'{i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1512.01274,
  doi = {10.48550/ARXIV.1512.01274},
  
  url = {https://arxiv.org/abs/1512.01274},
  
  author = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), Mathematical Software (cs.MS), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{liu2019deep,
  title={Deep learning accelerated light source experiments},
  author={Liu, Zhengchun and Bicer, Tekin and Kettimuthu, Rajkumar and Foster, Ian},
  booktitle={IEEE/ACM 3rd Workshop on Deep Learning on Supercomputers},
  pages={20--28},
  year={2019},
  organization={IEEE}
}

@article{liu2022exploring,
  title={Exploring physics of ferroelectric domain walls in real time: Deep learning enabled scanning probe microscopy},
  author={Liu, Yongtao and Kelley, Kyle P and Funakubo, Hiroshi and Kalinin, Sergei V and Ziatdinov, Maxim},
  journal={Advanced Science},
  pagesX={2203957},
  year={2022},
  publisher={Wiley Online Library}
}

@inproceedings{patton2018167,
  title={167-{P}flops deep learning for electron microscopy: From learning physics to atomic manipulation},
  author={Patton, Robert M and Johnston, J Travis and Young, Steven R and Schuman, Catherine D and March, Don D and Potok, Thomas E and Rose, Derek C and Lim, Seung-Hwan and Karnowski, Thomas P and Ziatdinov, Maxim A},
  booktitle={SC'18},
  pages={638--648},
  year={2018},
  organization={IEEE}
}

@inproceedings{chen2018tvm,
  title={TVM: An automated end-to-end optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis},
  booktitle={13th USENIX Symp.\ Operating Systems Design \& Impl.},
  pages={578--594},
  year={2018}
}

@misc{https://doi.org/10.48550/arxiv.1805.00907,
  doi = {10.48550/ARXIV.1805.00907},
  
  url = {https://arxiv.org/abs/1805.00907},
  
  author = {Rotem, Nadav and Fix, Jordan and Abdulrasool, Saleem and Catron, Garret and Deng, Summer and Dzhabarov, Roman and Gibson, Nick and Hegeman, James and Lele, Meghan and Levenstein, Roman and Montgomery, Jack and Maher, Bert and Nadathur, Satish and Olesen, Jakob and Park, Jongsoo and Rakhov, Artem and Smelyanskiy, Misha and Wang, Man},
  
  keywords = {Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Glow: Graph Lowering Compiler Techniques for Neural Networks},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{https://doi.org/10.48550/arxiv.1809.02697,
  doi = {10.48550/ARXIV.1809.02697},
  
  url = {https://arxiv.org/abs/1809.02697},
  
  author = {Liu, Yizhi and Wang, Yao and Yu, Ruofei and Li, Mu and Sharma, Vin and Wang, Yida},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Optimizing CNN Model Inference on CPUs},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE {9664259,
author = {S. Zheng and R. Chen and Y. Jin and A. Wei and B. Wu and X. Li and S. Yan and Y. Liang},
journal = {IEEE Transactions on Parallel and Distributed Systems},
title = {NeoFlow: A Flexible Framework for Enabling Efficient Compilation for High Performance DNN Training},
year = {2022},
volume = {33},
number = {11},
issn = {1558-2183},
pages = {3220-3232},
abstract = {Deep neural networks (DNNs) are increasingly deployed in various image recognition and natural language processing applications. The continuous demand for accuracy and high performance has led to innovations in DNN design and a proliferation of new operators. However, existing DNN training frameworks such as PyTorch and TensorFlow only support a limited range of operators and rely on hand-optimized libraries to provide efficient implementations for these operators. To evaluate novel neural networks with new operators, the programmers have to either replace the holistic new operators with existing operators or provide low-level implementations manually. Therefore, a critical requirement for DNN training frameworks is to provide high-performance implementations for the neural networks containing new operators automatically in the absence of efficient library support. In this article, we introduce NeoFlow, which is a flexible framework for enabling efficient compilation for high-performance DNN training. NeoFlow allows the programmers to directly write customized expressions as new operators to be mapped to graph representation and low-level implementations automatically, providing both high programming productivity and high performance. First, NeoFlow provides expression-based automatic differentiation to support customized model definitions with new operators. Then, NeoFlow proposes an efficient compilation system that partitions the neural network graph into subgraphs, explores optimized schedules, and generates high-performance libraries for subgraphs automatically. Finally, NeoFlow develops an efficient runtime system to combine the compilation and training as a whole by overlapping their execution. In the experiments, we examine the numerical accuracy and performance of NeoFlow. The results show that NeoFlow can achieve similar or even better performance at the operator and whole graph level for DNNs compared to deep learning frameworks. Especially, for novel networks training, the geometric mean speedups of NeoFlow to PyTorch, TensorFlow, and CuDNN are 3.16X, 2.43X, and 1.92X, respectively.},
keywords = {training;libraries;convolution;codes;deep learning;tensors;schedules},
doiX = {10.1109/TPDS.2021.3138862}
}


@inproceedings{maleki2011evaluation,
  title={An evaluation of vectorizing compilers},
  author={Maleki, Saeed and Gao, Yaoqing and Garzar, Maria J and Wong, Tommy and Padua, David A},
  booktitle={International Conference on Parallel Architectures and Compilation Techniques},
  pages={372--382},
  year={2011},
  organization={IEEE}
}

@misc{https://doi.org/10.48550/arxiv.1604.06174,
  doi = {10.48550/ARXIV.1604.06174},
  
  url = {https://arxiv.org/abs/1604.06174},
  
  author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training Deep Nets with Sublinear Memory Cost},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Duarte_2018,
	doiX = {10.1088/1748-0221/13/07/p07027},
  
	urlX = {https://doi.org/10.1088%2F1748-0221%2F13%2F07%2Fp07027},
  
	year = 2018,
  
	publisher = {{IOP} Publishing},
  
	volume = {13},
  
	number = {07},
  
	pages = {P07027--P07027},
  
	author = {J. Duarte and S. Han and P. Harris and S. Jindariani and E. Kreinar and B. Kreis and J. Ngadiuba and M. Pierini and R. Rivera and N. Tran and Z. Wu},
  
	title = {Fast inference of deep neural networks in {FPGAs} for particle physics},
  
	journal = {Journal of Instrumentation}
}

@ARTICLE{7368920,  author={Nane, Razvan and Sima, Vlad-Mihai and Pilato, Christian and Choi, Jongsok and Fort, Blair and Canis, Andrew and Chen, Yu Ting and Hsiao, Hsuan and Brown, Stephen and Ferrandi, Fabrizio and Anderson, Jason and Bertels, Koen},  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},   title={A Survey and Evaluation of FPGA High-Level Synthesis Tools},   year={2016},  volume={35},  number={10},  pages={1591-1604},  doi={10.1109/TCAD.2015.2513673}}


@misc{https://doi.org/10.48550/arxiv.2203.08402,
  doi = {10.48550/ARXIV.2203.08402},
  
  url = {https://arxiv.org/abs/2203.08402},
  
  author = {Hattori, Momoko and Kobayashi, Naoki and Sato, Ryosuke},
  
  keywords = {Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences, D.2.4},
  
  title = {Gradual Tensor Shape Checking},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@incollection{rajopadhye2002dependence,
  title={Dependence Analysis and Parallelizing Transformations},
  author={Rajopadhye, Sanjay V},
  booktitle={The Compiler Design Handbook},
  year={2002}
}

@article{10.1145/3296979.3192413,
author = {Moll, Simon and Hack, Sebastian},
title = {Partial Control-Flow Linearization},
year = {2018},
issue_date = {April 2018},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192413},
doi = {10.1145/3296979.3192413},
abstract = {If-conversion is a fundamental technique for vectorization. It accounts for the fact that in a SIMD program, several targets of a branch might be executed because of divergence. Especially for irregular data-parallel workloads, it is crucial to avoid if-converting non-divergent branches to increase SIMD utilization. In this paper, we present partial linearization, a simple and efficient if-conversion algorithm that overcomes several limitations of existing if-conversion techniques. In contrast to prior work, it has provable guarantees on which non-divergent branches are retained and will never duplicate code or insert additional branches. We show how our algorithm can be used in a classic loop vectorizer as well as to implement data-parallel languages such as ISPC or OpenCL. Furthermore, we implement prior vectorizer optimizations on top of partial linearization in a more general way. We evaluate the implementation of our algorithm in LLVM on a range of irregular data analytics kernels, a neutronics simulation benchmark and NAB, a molecular dynamics benchmark from SPEC2017 on AVX2, AVX512, and ARM Advanced SIMD machines and report speedups of up to 146 % over ICC, GCC and Clang O3.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {543-556},
numpages = {14},
keywords = {SPMD, Compiler optimizations, SIMD}
}


@inproceedings{10.1145/3192366.3192413,
author = {Moll, Simon and Hack, Sebastian},
title = {Partial Control-Flow Linearization},
year = {2018},
isbn = {9781450356985},
url = {https://doi.org/10.1145/3192366.3192413},
doi = {10.1145/3192366.3192413},
abstract = {If-conversion is a fundamental technique for vectorization. It accounts for the fact that in a SIMD program, several targets of a branch might be executed because of divergence. Especially for irregular data-parallel workloads, it is crucial to avoid if-converting non-divergent branches to increase SIMD utilization. In this paper, we present partial linearization, a simple and efficient if-conversion algorithm that overcomes several limitations of existing if-conversion techniques. In contrast to prior work, it has provable guarantees on which non-divergent branches are retained and will never duplicate code or insert additional branches. We show how our algorithm can be used in a classic loop vectorizer as well as to implement data-parallel languages such as ISPC or OpenCL. Furthermore, we implement prior vectorizer optimizations on top of partial linearization in a more general way. We evaluate the implementation of our algorithm in LLVM on a range of irregular data analytics kernels, a neutronics simulation benchmark and NAB, a molecular dynamics benchmark from SPEC2017 on AVX2, AVX512, and ARM Advanced SIMD machines and report speedups of up to 146 % over ICC, GCC and Clang O3.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {543-556},
numpages = {14},
keywords = {SIMD, SPMD, Compiler optimizations},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}


@inproceedings{10.1145/3174243.3174268,
author = {Dai, Steve and Liu, Gai and Zhang, Zhiru},
title = {A Scalable Approach to Exact Resource-Constrained Scheduling Based on a Joint SDC and SAT Formulation},
year = {2018},
isbn = {9781450356145},
urlX = {https://doi.org/10.1145/3174243.3174268},
doiX = {10.1145/3174243.3174268},
abstract = {Despite increasing adoption of high-level synthesis (HLS) for its design productivity advantage, success in achieving high quality-of-results out-of-the-box is often hindered by the inexactness of the common HLS optimizations. In particular, while scheduling forms the algorithmic core to HLS technology, current scheduling algorithms rely heavily on fundamentally inexact heuristics that make ad hoc local decisions and cannot accurately and globally optimize over a rich set of constraints. To tackle this challenge, we propose a scheduling formulation based on system of integer difference constraints (SDC) and Boolean satisfiability (SAT) to exactly handle a variety of scheduling constraints. We develop a specialized scheduler based on conflict-driven learning and problem-specific knowledge to optimally and efficiently solve the resource-constrained scheduling problem. By leveraging the efficiency of SDC algorithms and scalability of modern SAT solvers, our scheduling technique is able to achieve on average over 100x improvement in runtime over the integer linear programming (ILP) approach while attaining optimal latency. By integrating our scheduling formulation into a state-of-the-art open-source HLS tool, we further demonstrate the applicability of our scheduling technique with a suite of representative benchmarks targeting FPGAs.},
booktitle = {ACM/SIGDA Intl Symposium on Field-Programmable Gate Arrays},
pages = {137-146},
numpages = {10},
keywords = {satisfiability modulo theory, optimal solution, ilp, exact algorithm, resource-constrained scheduling, system of difference constraints, integer linear programming, rcs, hls, efficiency, satisfiability, sdc, high-level synthesis, scalability, problem-specific knowledge, conflict-driven learning, sat, scheduling},
}

@article{baruch1996scheduling,
  title={Scheduling algorithms for high-level synthesis},
  author={Baruch, Zoltan},
  journal={ACAM Scientific Journal},
  volume={5},
  number={1-2},
  pages={48--57},
  year={1996}
}

@misc{rosser2018cocotb,
  title={Cocotb: a Python-based digital logic verification framework},
  author={Rosser, Benjamin John},
  year={2018},
  publisher={CERN},
  note={\url{https://docs.cocotb.org}}
}

@misc{williamsicarus,
  title={Icarus Verilog, 1998--2020},
  author={Williams, Stephen},
  note={\url{http://iverilog.icarus.com}}
}

@article{Liu:fs5198,
	author = {Liu, Zhengchun and Sharma, Hemant and Park, Jun-Sang and Kenesei, Peter and Miceli, Antonino and Almer, Jonathan and Kettimuthu, Rajkumar and Foster, Ian},
	journal = {IUCrJ},
	number = {1},
	pages = {104--113},
	title = {{{\it BraggNN}: fast X-ray Bragg peak analysis using deep learning}},
	volume = {9},
	year = {2022}}


@techreport{guideultrascale,
  title={UltraScale Architecture DSP Slice},
  url={https://docs.xilinx.com/v/u/en-US/ug579-ultrascale-dsp},
  institution={XiLinx},
  year={2021}
}

@article{leibson2013xilinx,
  title={Xilinx ultrascale: The next-generation architecture for your next-generation architecture},
  author={Leibson, Steve and Mehta, Nick},
  journal={Xilinx White Paper WP435},
  volume={143},
  year={2013}
}

@misc{rapidwright,
  title = {Create placed and routed DCP to cross SLR},
  howpublished = {\url{https://www.rapidwright.io/docs/SLR_Crosser_DCP_Creator_Tutorial.html}},
  note = {Accessed: 2022-10-15}
}

@inproceedings{oppermann2022eurollvm,
    author = {Oppermann, Julian and Urbach, Mike and Demme, John}, 
    title = {How to make hardware with maths: An introduction to {CIRCT}'s scheduling infrastructure},
    booktitle = {European {LLVM} {Developers}' {Meeting}},
    year = {2022}
}

@misc{https://doi.org/10.48550/arxiv.1502.03167,
  doi = {10.48550/ARXIV.1502.03167},

  url = {https://arxiv.org/abs/1502.03167},

  author = {Ioffe, Sergey and Szegedy, Christian},

  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},

  publisher = {arXiv},

  year = {2015},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{daceml,
 author = {Rausch, Oliver},
 title = {{DaCeML}: A Data-Centric Optimization Framework for Machine Learning},
  year = {2022},
 booktitle = {36th ACM International Conference on Supercomputing},
}

@INPROCEEDINGS{
    7783720,
    author={Sharma, Hardik},
    booktitle={49th Annual IEEE/ACM International Symposium on Microarchitecture},
    title={From high-level deep neural models to FPGAs},
    year={2016},
    volume={},
    number={},
    pages={1-12},
    doiX={10.1109/MICRO.2016.7783720}
}

@ARTICLE{9786533,
  author={Bohm Agostini, Nicolas},
  journal={IEEE Micro},
  title={Bridging Python to Silicon: The SODA Toolchain},
  year={2022},
  doi={10.1109/MM.2022.3178580}}

@inproceedings{takamaeda2015pyverilog,
  title={Pyverilog: A Python-based hardware design processing toolkit for Verilog HDL},
  author={Takamaeda-Yamazaki, Shinya},
  booktitle={International Symposium on Applied Reconfigurable Computing},
  pages={451--460},
  year={2015},
  organization={Springer}
}

@inproceedings{10.1145/3431920.3439289,
author = {Guo, Licheng},
title = {AutoBridge: Coupling Coarse-Grained Floorplanning and Pipelining for High-Frequency HLS Design on Multi-Die FPGAs},
year = {2021},
isbn = {9781450382182},
urlX = {https://doi.org/10.1145/3431920.3439289},
doiX = {10.1145/3431920.3439289},
abstract = {Despite an increasing adoption of high-level synthesis (HLS) for its design productivity advantages, there remains a significant gap in the achievable clock frequency between an HLS-generated design and a handcrafted RTL one. A key factor that limits the timing quality of the HLS outputs is the difficulty in accurately estimating the interconnect delay at the HLS level. Unfortunately, this problem becomes even worse when large HLS designs are implemented on the latest multi-die FPGAs, where die-crossing interconnects incur a high delay penalty.To tackle this challenge, we propose AutoBridge, an automated framework that couples a coarse-grained floorplanning step with pipelining during HLS compilation. First, our approach provides HLS with a view on the global physical layout of the design, allowing HLS to more easily identify and pipeline the long wires, especially those crossing the die boundaries. Second, by exploiting the flexibility of HLS pipelining, the floorplanner is able to distribute the design logic across multiple dies on the FPGA device without degrading clock frequency. This prevents the placer from aggressively packing the logic on a single die which often results in local routing congestion that eventually degrades timing. Since pipelining may introduce additional latency, we further present analysis and algorithms to ensure the added latency will not compromise the overall throughput.AutoBridge can be integrated into the existing CAD toolflow for Xilinx FPGAs. In our experiments with a total of 43 design configurations, we improve the average frequency from 147 MHz to 297 MHz (a 102\% improvement) with no loss of throughput and a negligible change in resource utilization. Notably, in 16 experiments we make the originally unroutable designs achieve 274 MHz on average. The tool is available at https://github.com/Licheng-Guo/AutoBridge.},
booktitle = {ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {81–92},
numpages = {12},
keywords = {latency insensitive design, dataflow, timing closure, frequency, pipeline, hls, high-level synthesis, floorplan, multi-die fpga}
}