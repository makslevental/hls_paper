\documentclass[sigconf,techreport]{acmart}

\input{preamble.tex}

\newif\iffinal
\finalfalse


\iffinal
	\newcommand{\maxx}[1]{}
	\newcommand{\ryan}[1]{}
	\newcommand{\kyle}[1]{}
	\newcommand{\ian}[1]{}
	\newcommand{\arham}[1]{}
	\newcommand{\commnt}[2]{#2}
\else
	\newcommand{\maxx}[1]{{\textcolor{red}{ Max: #1 }}}
	\newcommand{\ryan}[1]{{\textcolor{magenta}{ Ryan: #1 }}}
	\newcommand{\kyle}[1]{{\textcolor{yellow}{ Kyle: #1 }}}
	\newcommand{\ian}[1]{{\textcolor{orange}{ Ian: #1 }}}
	\newcommand{\arham}[1]{{\textcolor{pink}{ Arham: #1 }}}
	\newcommand{\commnt}[2]{{{\color{green} \{#1\}} {\color{blue} #2}}}
\fi


\begin{document}

\title{BraggHLS}


\author{Maksim Levental, Arham, Kaz, Ryan "the champ", Kyle, and Ian Foster}
\affiliation{%
	\institution{University of Chicago}
	\department{Department of Computer Science}
	\city{Chicago}
	\state{Illinois}
	\postcode{60637}
	\country{USA}
}
\email{{mlevental,..,foster}@uchicago.edu}

\renewcommand{\shortauthors}{Levental et al.}


\begin{abstract}
	In many experiment-driven scientific domains, such as particle physics and X-ray crystallography, high sensor sample rates necessitate low latency, near-sensor, data processing.
	By replacing physics-based, but computationally intensive, computations with learned approximations, Deep neural network (DNN) methods can enable efficient implementations of these key data processing tasks.
	Despite this, few DNNs have been successfully deployed in such domains, owing to the inability of conventional DNN deployment platforms to meet hard latency \emph{and} collocation constraints.
	Here we present a case-study of implementing a particular DNN model for an alternative platform, namely Field Programmable Gate Arrays (FPGAs).
	We use state-of-the-art compiler techniques and optimization methods, along with suitable approximations, to implement a DNN for Bragg peak detection on a data-center class FPGA.
	Our implementation has comparable accuracy to conventional BraggNN and can achieve 3~\textmu s end-to-end latency: just a factor of three from the requisite 1~\textmu s.
\end{abstract}


%    \begin{CCSXML}
%    <ccs2012>
%    <concept>
%    <concept_id>
%        10002951.10003227.10003392</concept_id>
%        <concept_desc>Information systems~Digital libraries and archives</concept_desc>
%        <concept_significance>500</concept_significance>
%        </concept>
%        <concept>
%        <concept_id>10002951.10003260</concept_id>
%        <concept_desc>Information systems~World Wide Web</concept_desc>
%        <concept_significance>500</concept_significance>
%        </concept>
%        </ccs2012>
%    \end{CCSXML}

\ccsdesc[500]{Information systems~Digital libraries and archives}
\ccsdesc[500]{Information systems~World Wide Web}

% Comment the above block out to hide CCS Concepts or update as per https://dl.acm.org/ccs/ccs.cfm


\keywords{Memento, Web Archiving, Frogs}


\maketitle

\tableofcontents

\section{Introduction}\label{sec:introduction}
\input{introduction.tex}

\section{Background}\label{sec:background}
\input{background.tex}

\section{Five weird tricks accelerator manufacturers don't want you to know}\label{sec:methodology}
\input{methodology.tex}

\section{Evaluation}\label{sec:evaluation}
\input{evaluation.tex}

\section{Acknowledgements}

\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

\appendix

\section{MLIR Example}\label{sec:mlir_example}
\input{mlir_example.tex}

\end{document}
