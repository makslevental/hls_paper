%% LyX 2.3.6.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[twocolumn,conference]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{color}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=1,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{pdftitle={Your Title},
 pdfauthor={Your Name},
 pdfpagelayout=OneColumn, pdfnewwindow=true, pdfstartview=XYZ, plainpages=false}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% for subfigures/subtables
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage[dvipsnames]{xcolor}
\definecolor{lgray}{rgb}{0.95, 0.95, 0.95}

\@ifundefined{showcaptionsetup}{}{%
 \PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\makeatother

\usepackage{minted}
\renewcommand{\listingscaption}{Listing}

\begin{document}
\title{\texttt{BraggHLS}}
\author{\IEEEauthorblockN{Maksim~Levental}\IEEEauthorblockA{University of Chicago\\
Email: test@test.tes}\and \IEEEauthorblockN{Ryan~Chard}\IEEEauthorblockA{Ecole Superieure\\
Nantes, France\\
Email: second@second.fr}\and \IEEEauthorblockN{Kyle~Chard\\
and Ian~Foster}\IEEEauthorblockA{Star Academy\\
San Francisco, California 99999-9999\\
Telephone: (800) 555\textendash 5555\\
Fax: (888) 555\textendash 5555}}
\maketitle
\begin{abstract}
In many experiment-driven scientific domains, such as high-energy
physics, material science, and cosmology, very high data rate experiments
impose hard constraints on the corresponding data acquisition systems:
collected data must either be indiscriminately stored for post-processing
and analysis, thereby necessitating large storage capacity, or accurately
filtered in real-time, thereby necessitating low latency execution.
Deep neural networks, effective in many other filtering tasks, have
not been widely employed in such data acquisition systems, due to
design and deployment difficulties. This paper presents an open source,
lightweight, compiler framework \texttt{BraggHLS}, based on high-level
synthesis techniques, for translating high-level representations of
deep neural networks to low-level representations, suitable for deployment
to near-sensor devices such as field-programmable gate arrays. We
evaluate \texttt{BraggHLS} on various workloads and present a case-study
deep neural network for Bragg peak detection in the context of high-energy
diffraction microscopy. We show \texttt{BraggHLS} is able to produce
an implementation of the network with a throughput 4.8 \textmu s/sample,
which is approximately a 4$\times$ improvement over the existing
implementation.

\tableofcontents{}
\end{abstract}


\section{Introduction\label{sec:Introduction}}

Very high data rates are observed and, consequently, large datasets
are generated across a broad range of experiments in scientific domains,
such as high-energy physics, material science, and cosmology. For
example, in high-energy physics, the LHCb detector, at the CERN Large
Hadron Collider, is tasked with observing the trajectories of particles
produced in proton-proton collisions at a rate of 40 million per second
(i.e., 40 MHz) \cite{pmlr-v42-glig14}. With a packet size of approximately
50 kB (per collision), this implies a data rate of approximately 2
TB/s. Ultimately, in combination with other detectors, the LHC processes
approximately 100 EB of data a year. In materials science, high-energy
diffraction microscopy (HEDM) techniques, which provide non-destructive
characterization of structure and its evolution in a broad class of
single-crystal and polycrystalline materials, can have collection
rates approaching 1 MHz \cite{Hammer_2021}, with a corresponding
packet size of 80 kB. In cosmology, the Square Kilometer Array, a
radio telescope projected to be completed in 2024 and to be operational
by 2027 \cite{mcmullin2022square}, will sustain data rates in excess
of 10 TB/s \cite{grainge2017square}.

Naturally, for high data rate experiments, directly storing and distributing
such large quantities of data to the associated research communities
for further analysis is cost prohibitive. Thus, either compression
(in the case of storage and transmission) or outright filtering is
necessary, i.e., only a small fraction of the most ``interesting''
data is selected at time of collection, with the remainder being permanently
discarded. In this work we focus on the filtering approach. Note,
that the tradeoff made in employing filtering should be clear: reduced
storage at the expense of more stringent latency constraints (on the
filtering mechanisms). In addition, the risk of discarding meaningful
data introduces accuracy (of the filtering mechanisms) as a critical
new dimension of the data acquisition systems. Typically, these filtering
mechanisms consist either of physics based models \cite{LHCB-FIGURE-2020-018}
or machine learning models \cite{Gligorov_2013}; in either case maximally
efficient and effective use of the target hardware platform is tantamount
to accuracy. Irrespective of the type of technique employed, almost
universally, for the ultra-low latency use cases (e.g., sub-microsecond
latency constraints), the implementation is deployed to either field-programmable
gate arrays (FPGAs) or application-specific integrated circuits (ASICs)
\cite{Duarte_2018}. %
\begin{comment}
The reason for this is only FPGAs and ASICs are flexible enough to
satisfy the latency constraints for a wide range of techniques. Note,
in this work we focus exclusively on FPGAs.
\end{comment}

Deep neural networks (DNNs), a particular type of machine learning
model, have been shown to be effective in many scientific and commercial
domains due to their ``representational capacity'', i.e., they demonstrate
a capacity to (approximately) represent diverse sets of mappings \cite{alzubaidi2021review}.
DNNs ``learn'' to represent a mapping over the course of ``training'',
wherein they are iteratively evaluated on sample data while a ``learning
rule'' periodically updates the parameters (\emph{weights}) that
parameterize the DNN. In recent years they have been investigated
for near real-time scientific use cases \cite{liu2019deep,patton2018167,liu2022exploring}
but their use for the lowest latency use cases has been very limited
\cite{Duarte_2018}. The reasons for this are threefold: 
\begin{enumerate}
\item Graphics Processing Units (GPUs), the conventional hardware target
for DNNs, until very recently, have not been performant enough for
these very high data rate, very low latency, use cases (due to their
low clock speeds and low peripheral bandwidth \cite{aaij2020allen});
\item DNNs, by virtue of their depth, are resource intensive, in terms of
both memory (for the weights) and compute (floating point arithmetic),
thereby preventing their deployment to FPGAs, which, in particular,
have limited static RAM available;
\item DNNs are (typically) defined, trained, and distributed using high-level
frameworks (such as PyTorch \cite{paszke2017automatic}, TensorFlow
\cite{https://doi.org/10.48550/arxiv.1603.04467}, MXNet \cite{https://doi.org/10.48550/arxiv.1512.01274}),
which abstract all implementation details from the user, thereby making
portability of existing model architectures (to e.g., FPGA) nigh impossible.
\end{enumerate}
These three barriers demand of a solution that can simultaneously
translate a high-level representation of a DNN to a low-level representation,
suitable for deployment to FPGA, while optimizing resource usage and
minimizing latency. In general, the task of \emph{lowering} high-level
representations of programs to lower-level representations is the
domain of a compiler. Similarly, the task of \emph{synthesizing} a\emph{
register-transfer level} (RTL) \emph{design}, rendered in a \emph{hardware
description language} (HDL), from a program, is the domain of high-level
synthesis (HLS) \cite{7368920}. While several such HLS tools exist
\cite{10.1145/2514740,Zhang2008,ferrandi2021bambu} and despite, often,
bundling robust optimizing compilers, they struggle to effectively
perform the necessary optimizations in reasonable amounts of time
(see Section \ref{sec:Evaluation}).

Recently, deep learning compilers (such as TVM \cite{chen2018tvm},
MLIR \cite{https://doi.org/10.48550/arxiv.2002.11054}, and Glow \cite{https://doi.org/10.48550/arxiv.1805.00907})
have demonstrated the ability to dramatically reduce inference latencies
\cite{https://doi.org/10.48550/arxiv.1809.02697}, training times
\cite{9664259}, and memory usage \cite{https://doi.org/10.48550/arxiv.1604.06174}
of DNNs. These compilers function by extracting intermediate-level
representations (IRs) of the DNNs, from the representations produced
by the frameworks, and performing various optimizations on those IRs
(such as kernel fusion \cite{10.1145/2858788.2688521}, vectorization
\cite{maleki2011evaluation}, and memory planning \cite{https://doi.org/10.48550/arxiv.1604.06174}).
The highly optimized IR is then used to generate code for various
target hardware platforms. Given the successes of these compilers,
it's natural to wonder whether they can adapted to the task of sufficiently
optimizing a DNN such that it might be synthesized to RTL, for deployment
to FPGA.

In this paper, we present \texttt{BraggHLS}, an open source, lightweight,
compiler and HLS framework which can lower DNNs defined as PyTorch
models to FPGA implementations. \texttt{BraggHLS} uses a combination
of compiler and HLS techniques to compile the entire DNN into a \emph{statically
scheduled} circuit, thereby eliminating all synchronization overheads
and achieving ultra-low latency. \texttt{BraggHLS} is general and
supports a wide range of DNN layer types, and thus a wide range of
DNNs, but particularly focus on a DNN designed for identifying Bragg
diffraction peaks. In summary our specific contributions include:
\begin{enumerate}
\item We discuss the challenges faced by a compiler and HLS tool in attempting
to lower DNNs to ultra-low latency designs, including runtime costs
incurred during design space exploration, challenges meeting resource
and timing constraints during synthesis, placement, and routing;
\item We describe and implement a compiler framework, \texttt{BraggHLS},
which can effectively transform unoptimized, hardware-agnostic PyTorch
models into ultra-low latency RTL designs suitable for deployment
to Xilinx FPGAs. \texttt{BraggHLS} is thoroughly tested, open source,
and available at \href{https://github.com/makslevental/bragghls/}{https://github.com/makslevental/bragghls/};
\item We show that designs generated by \texttt{BraggHLS} achieve lower
latency than Xilinx's state-of-the-art commercial HLS tool (Vitis
HLS) for a variety of DNN layer types. In particular we show that
\texttt{BraggHLS} can produce synthesizable designs that meet placement,
routing, and timing constraints, where Vitis HLS cannot.
\end{enumerate}
The rest of this paper is organized as follows: Section \ref{sec:Background}
reviews key concepts from compilers, high-level synthesis, and FPGA
design flows. Section \ref{sec:BraggHLS-compiler-and} describes the
\texttt{BraggHLS} compiler and HLS framework in detail. Section \ref{sec:Evaluation}
evaluates \texttt{BraggHLS}\textquoteright s performance, scalability,
and competitiveness with designs generated by Vitis HLS. Section \ref{sec:BraggNN-case-study}
describes our case study, \texttt{BraggHLS} applied to \texttt{BraggNN},
a Bragg peak detection DNN with a target latency of 1\textmu s/sample.
Finally, Section \ref{sec:Conclusion} concludes with a summary, and
related and future work.

\section{Background\label{sec:Background}}

\subsection{Compilers: the path from high to low}

The path from a high-level, abstract, representations of a DNN to
a register-transfer level representation can be neatly formulated
as a series of progressive lowerings between adjacent levels of abstraction.
Each level of abstraction is rendered as a programming language, IR,
or HDL, and thus we describe each lowering in terms the representations
and tools \texttt{BraggHLS} employs:
\begin{enumerate}
\item An imperative, \emph{define-by-run,} Python representation, in PyTorch;
\item High-level data-flow graph representation, in TorchScript;
\item Low-level data and control flow graph representation, in MLIR.
\end{enumerate}
%

\subsubsection{PyTorch and TorchScript}

Typically DNN models are represented in terms of high-level frameworks,
themselves implemented within general purpose programming languages.
Such frameworks are widely used because of their ease of use and large
library of example implementations of various DNN model architectures.
\texttt{BraggHLS} is implemented using PyTorch, thus we focus on relevant
aspects of PyTorch. DNNs developed within PyTorch are \emph{defined-by-run}:
the author imperatively describes the DNN in terms of high-level operations,
using python, which when executed materializes the high-level data-flow
graph (DFG) corresponding to the DNN (e.g., for the purposes of reverse-mode
automatic differentiation). From the perspective of the user, define-by-run
enables fast iteration at development time, possibly at the cost of
some runtime performance. 

From the perspective of compilation, define-by-run precludes efficient
extraction of the high-level DFG; since the DFG is materialized only
at runtime, it cannot be inferred from the textual representation
(i.e., the python source) of the DNN. Furthermore, apriori, the runtime-materialized
DFG is only partially materialized\footnote{``...instead, every intermediate result records only the subset of
the computation graph that was relevant to their computation.'' \cite{paszke2017automatic}}, and only as an in-memory data structure. Thus, framework support
is necessary. Indeed, PyTorch supports a Single Static Assignment
(SSA) IR, called TorchScript (TS) IR and accompanying tracing mechanism
(the TS JIT) to produce TS IR from conventionally defined PyTorch
models. Lowering from PyTorch to TS IR enables various useful analyses
and transformations on a DNN at the level of the high-level DFG (such
as kernel fusion \cite{10.1145/2858788.2688521}) but targeting FPGAs
requires a broader collection of transformations. To this end, we
turn to a recent addition to the compiler ecosystem.

\subsubsection{MLIR}

Multi-level Intermediate Representation \cite{https://doi.org/10.48550/arxiv.2002.11054}
presents a new approach to building reusable and extensible compiler
infrastructure. MLIR is composed of a set of \emph{dialect} IRs, subsets
of which are mutually compatible, either outright or by way of translation/legalization.
The various dialects aim to capture and formalize the semantics of
compute intensive programs at varying levels of abstraction, as well
as namespace related sets of IR transformations. The entrypoint into
this compiler framework, from PyTorch, is the \texttt{torch} dialect
\cite{torch-mlir}, a high-fidelity mapping from TS IR to MLIR native
IR, which, in addition to performing the translation to MLIR, fully
refines all shapes of intermediate tensors in the DNN (i.e., computes
concrete values for all dimensions of each tensor); this is necessary
for downstream optimizations and eliminating inconsistencies in the
DNN \cite{https://doi.org/10.48550/arxiv.2203.08402}.

While the \texttt{torch} dialect is necessary for lowering to MLIR
and shape refinement, it is a representation of a DNN at the same
level of abstraction as TS IR: it does not capture the precise data
flow and control flow necessary for novel implementations of DNN operations
(e.g., for FPGA). Fortunately, MLIR supports lower-level dialects,
such as the \texttt{linalg}, \texttt{affine} and \texttt{scf} (structured
control flow) dialects. The \texttt{scf} dialect is a straightforward
formalization of control flow primitives, such as conditionals and
loops. The \texttt{affine} dialect, on the other hand, provides a
formalization of semantics that lend themselves to polyhedral compilation
techniques \cite{polyhedral-mlir}, i.e., techniques that enable loop
dependence analysis and loop transformations. Such loop transformations,
particularly loop unrolling, are crucial for achieving lowest possible
latencies \cite{yehpca2022scalehls}.

\subsection{High-level synthesis and FPGA design}

\subsubsection{High-level synthesis\label{subsec:High-level-synthesis}}

High-level synthesis tools produce RTL descriptions of digital designs
from high-level representations, such as C or C++ \cite{10.1145/2514740,ferrandi2021bambu}.
In particular, Xilinx's Vitis HLS, based on the Autopilot project
\cite{Zhang2008}, is a state-of-the-art HLS tool. Given a high-level,
procedural, representation, HLS proceeds in three steps, in order
to produce a corresponding RTL design:
\begin{enumerate}
\item HLS schedules operations (such as \texttt{mulf}, \texttt{addf}, \texttt{load},
\texttt{store}) in order to determine which operations should occur
during each clock cycle. Such a schedule depends on three characteristics
of the high-level representation:
\begin{enumerate}
\item The topological ordering of the DFG/CFG of the procedural representation
(i.e., the dependencies of operations on results of other operations
and resources);
\item The completion time for each operation;
\item The user's desired clock rate/frequency;
\end{enumerate}
\item HLS associates operations to particular RTL instantiations (called
\emph{binding}) for those operations; for example whether to associate
an add operation followed by a multiply operation to two separate
instances, or whether to associate them both with a single instance,
e.g., conFigured to perform a fused multiply-accumulate (MAC);
\begin{enumerate}
\item In the case of floating point arithmetic operations, HLS also (with
user guidance) determines the precision of the floating point representation;
\end{enumerate}
\item HLS builds a finite-state machine (FSM) that implements the schedule
of operations as control logic, i.e., logic that initiates operations
and routes signals between them during the appropriate FSM stages.
\end{enumerate}
In addition to fulfilling these three fundamental tasks, high-level
synthesis aims to optimize the program, during synthesis. In particular,
they try to maximize concurrency and parallelism (number of concurrent
operations scheduled during a clock-cycle) in order maximize the throughput
and minimize the latency of the final implementation. Maximizing concurrency
entails pipelining operations: operations are executed such that they
overlap in time, subject to available resources. Maximizing parallelism
entails partitioning the DNN into subsets of operation that can be
computed independently and simultaneously and whose results are combined
upon completion. 

Both maximizing for concurrency and parallelism necessitates data-flow
analysis in order to identify data dependencies amongst operations,
which would lead to data hazards in synthesized designs. Such data-flow
analysis is expensive and grows (in runtime) as higher performance
is pursued. This can be understood in terms of loop-nest representations
of DNN operations; for example consider a convolution as in Listing
\ref{lis:Single-filter-convolution}.
\begin{listing}
\begin{minted}[fontsize={\footnotesize},escapeinside={||},mathescape=true]{python}
def conv2d(
  input: array(|$b$|, |$c_{in}$|, |$h$|, |$w$|),
  output: array(|$b$|, |$c_{out}$|, |$h$|, |$w$|),
  weight: array(|$c_{out}$|, |$c_{in}$|, |$k$|, |$k$|)
):
  for iv1 in range(0, |$b$|):
    for iv2 in range(0, |$c_{out}$|):
      for iv3 in range(0, |$h$|):
        for iv4 in range(0, |$w$|):
          for iv5 in range(0, |$c_{in}$|):
            for iv6 in range(0, |$k$|):
              for iv7 in range(0, |$k$|):
                _3 = iv3 + iv6
                _4 = iv4 + iv7
                _5 = input[iv1, iv5, _3, _4]
                _6 = weight[iv2, iv5, iv6, iv7]
                _7 = output[iv1, iv2, iv3, iv4]
                _8 = _5 * _6
                _9 = _7 + _8
                output[iv1, iv2, iv3, iv4] = _9
\end{minted}
\caption{Padding $\left\lfloor k/2\right\rfloor $, stride 1, $c_{out}$ filter
convolution with $k\times k$ kernel applied to ($\ensuremath{b},\ensuremath{c_{in}},\ensuremath{h},\ensuremath{w}$)-dimensional\texttt{
input} tensor, where $b$ is the batch size, $c_{in}$ is the number
of channels, and ($h,w$) are the height and width, respectively.\label{lis:Single-filter-convolution}}
\end{listing}
 A schedule for the arithmetic operations for this loop nest can be
computed by first unrolling all the loops up to some ``trip-count''
and then computing the topological sort of the operations (known as
\emph{list scheduling}). The degree to which the loops are unrolled
determines how many arithmetic operations can be scheduled in parallel.
The issue is that the stores and loads on the \texttt{output} array
prevent reconstruction of explicit relationships between the inputs
and outputs of the arithmetic operations across loop iterations. The
standard resolution is to perform \emph{store-load forwarding}: pairs
of store and load operations to/from the same memory address are eliminated,
with the operand of the store forwarded to the uses of the load (see
Listing \ref{lis:Single-filter-convolution-1}).
\begin{listing}
\begin{minted}[numbers=left,fontsize={\footnotesize},escapeinside={||},mathescape=true,highlightlines={19,25},autogobble=true,numbersep=3pt]{python}
def conv2d(
  input: array(|$b$|, |$c_{in}$|, |$h$|, |$w$|),
  output: array(|$b$|, |$c_{out}$|, |$h$|, |$w$|),
  weight: array(|$c_{out}$|, |$c_{in}$|, |$k$|, |$k$|)
):
  for iv1 in range(0, |$b$|):
    for iv2 in range(0, |$c_{out}$|):
      for iv3 in range(0, |$h$|):
        for iv4 in range(0, |$w$|):
	  ...
	  # e.g., iv5, iv6, iv7 = 2, 3, ${\setlength{\fboxsep}{1pt}\colorbox{Salmon}{\texttt{4}}}$
	  _31 = iv3 + iv6 
	  _41 = iv4 + iv7 
	  _51 = input[iv1, iv5, _31, _41] 
	  _61 = weight[iv2, iv5, iv6, iv7] 
	  _71 = output[iv1, iv2, iv3, iv4] 
	  _81 = _51 * _61 
	  |${\setlength{ \fboxsep}{1pt} \colorbox{green}{ \texttt{\_91}}}$| = _71 + _81 
	  output[iv1, iv2, iv3, iv4] = _91 
	  # iv5, iv6, iv7 = 2, 3, ${\setlength{\fboxsep}{1pt}\colorbox{Salmon}{\texttt{5}}}$
	  _32 = iv3 + iv6 
	  _42 = iv4 + iv7 
	  _52 = input[iv1, iv5, _32, _42] 
	  _62 = weight[iv2, iv5, iv6, iv7] 
	  |${\setlength{\fboxsep}{1pt}\colorbox{yellow}{\texttt{\_72}}}$| = output[iv1, iv2, iv3, iv4] 
	  _82 = _52 * _62 
	  |${\setlength{\fboxsep}{1pt}\colorbox{Cyan}{\texttt{\_92}}}$| = |${\setlength{\fboxsep}{1pt}\colorbox{yellow}{\texttt{\_72}}}$| + _82 
	  output[iv1, iv2, iv3, iv4] = _92
	  ...
\end{minted}
\caption{Store-load forwarding across successive iterations (e.g.,\texttt{
iv7} $={\setlength{\fboxsep}{1pt}\colorbox{Salmon}{\texttt{4}}}, {\setlength{\fboxsep}{1pt}\colorbox{Salmon}{\texttt{5}}}$)
of the inner loop in Listing \ref{lis:Single-filter-convolution},
after unrolling. The forwarding opportunity is from the store on line
19 to the load on line 25; both can be eliminated and ${\setlength{\fboxsep}{1pt}\colorbox{green}{\texttt{\_91}}}$
can replace uses of ${\setlength{\fboxsep}{1pt}\colorbox{yellow}{\texttt{\_72}}}$,
such as in the computation of \texttt{${\setlength{\fboxsep}{1pt}\colorbox{Cyan}{\texttt{\_92}}}$}
(and potentially many others).\label{lis:Single-filter-convolution-1}}
\end{listing}
 In order for this transformation to be correct (preserve program
semantics), for each pair of candidate store and load operations,
it must be verified that there are no intervening memory operations.
Note, the number of such checks scales polynomially in the parameters
of the convolution since the loop nest unrolls into $b\times c_{out}\times h\times w\times c_{in}\times k^{2}$
store-load pairs. Note also, while in the case of Listing \ref{lis:Single-filter-convolution}
the verification is straightforward, in general it might involve solving
a small constraint satisfaction program \cite{rajopadhye2002dependence}.

Finally, note, though greedy solutions to the scheduling problem solved
by HLS are possible, in principle scheduling is an integer linear
programming problem (ILP), instances of which are NP-hard. In summary,
HLS tools solve computationally intensive problems in order to produce
a RTL description of a high-level representation of a DNN. These phases
of the HLS process incur ``development time'' costs (i.e., runtime
of the tools) and impose practical limitations on the amount of design
space exploration (for the purpose of achieving latency goals) which
can be performed. \texttt{BraggHLS }addresses these issues by enabling
the user to employ heuristics (during both the parallelization and
scheduling phases) which, while not guaranteed to be correct, can
be \emph{behaviorally verified} (see Section \ref{subsec:AST-transformations-and}).

\subsection{FPGA design}

At the register-transfer level of abstraction, there remain two more
steps prior to being able to actually deploy to an FPGA; one of them
being a final lowering, so-called logic synthesis, and the other being
place and route (P\&R). The entire process is carried out, for example,
by Xilinx's Vivado tool. 

Logic synthesis is the process of mapping RTL to actual hardware primitives
on the FPGA (so-called \emph{technology mapping}), such as lookup
tables (LUTs), block RAMs (BRAMs), flip-flops (FFs), and digital signal
processors (DSPs). Logic synthesis produces a network list (\emph{netlist})
describing the logical connectivity of various parts of the design.
Logic synthesis effectively determines the implementation of floating
point operations in terms of DSPs; depending on user parameters and
other design features, DSP resource consumption for floating point
multiplication and addition can differ greatly. The number of LUTs
and DSPs that a high-level representation of a DNN corresponds to
is relevant to both the performance and feasibility of that DNN when
deployed to FPGA.

After the netlist has been produced, the entire design undergoes P\&R.
The goal of P\&R is to determine which configurable logic block within
an FPGA should implement each of the units of logic required by the
digital design. P\&R algorithms need to minimize distances between
related units of functionality (in order to minimize wire delay),
balance wire density across the entire fabric of the FPGA (in order
to reduce route congestion), and maximize the clock speed of the design
(a function of both wire delay, logic complexity, and route congestion).
The final, routed design, can then be deployed to the FPGA by producing
a proprietary \emph{bitstream}, which is written to the FPGA.

\section{\texttt{BraggHLS} compiler and HLS framework\label{sec:BraggHLS-compiler-and}}

\texttt{BraggHLS} is an extensible (implemented in python) HLS framework
which employs MLIR for extracting loop-nest representations of DNNs
implemented as PyTorch models. Critically, and distinctly, it handles
the DNN transformations as well as scheduling, binding, and FSM extraction;
there is no dependence on any commercial HLS tools. Figure \ref{fig:BraggHLS-framework-overview.}
shows the architecture of \texttt{BraggHLS} 
\begin{figure}[tbh]
\centering{}\includegraphics[width=1\columnwidth]{../figures/BraggHLS}\caption{\texttt{BraggHLS} framework overview (placeholder).\label{fig:BraggHLS-framework-overview.}}
\end{figure}
. We discuss the most significant aspects of the architecture in the
following.

\subsection{Symbolic interpretation for fun and profit\label{subsec:Symbolic-execution-for}}

First, DNNs are lowered from PyTorch to MLIR through TorchScript and
the \texttt{torch} dialect. They are then further lowered from the
\texttt{torch} dialect to the \texttt{scf} dialect (through the \texttt{linalg}
dialect) in such a way that the inherent parallelism of each high-level
operation is preserved (for example, see Listing \ref{lis:Single-filter-convolution-2}).
\begin{listing}
\begin{minted}[fontsize={\footnotesize},escapeinside={||},mathescape=true]{mupad}
@conv2d(
    %input: memref<|$b \times c_{in} \times h \times w$|>,
    %weight: memref<|$b \times c_{out} \times h \times w$|>,
    %output: memref<|$c_{out} \times c_{in} \times k \times k$|>
) {
  scf.parallel (%iv1, %iv2, %iv3, %iv4) =
               (%c0, %c0, %c0, %c0) to
               (|$b$|, |$c_{out}$|, |$h$|, |$w$|) step
               (%c1, %c1, %c1, %c1) {
    scf.for %iv5 = %c0 to |$c_{in}$| step %c1 {
      scf.for %iv6 = %c0 to |$k$| step %c1 {
        scf.for %iv7 = %c0 to |$k$| step %c1 {
          %3 = arith.addi %iv3, %iv6
          %4 = arith.addi %iv4, %iv7
          %5 = memref.load %input[%iv1, %iv5, %iv3, %3, %4]
          %6 = memref.load %weight[%iv2, %iv5, %iv6, %iv7]
          %7 = memref.load %output[%iv1, %iv2, %iv3, %iv4]
          %8 = arith.mulf %5, %6
          %9 = arith.addf %7, %8
          memref.store %9, %output[%iv1, %iv2, %iv3, %iv4]
        }
      }
    }
  }
  return %2
}
\end{minted}
\caption{Parallel loop representation of the convolution in Listing \ref{lis:Single-filter-convolution}.\label{lis:Single-filter-convolution-2}}
\end{listing}
The value of making the parallelism explicit is that we can readily
partition a DNN across a known set of hardware resources in order
to maximize parallelism; since for each value of (\texttt{\%iv1},\texttt{
\%iv2},\texttt{ \%iv3},\texttt{ \%iv4}) the body of the \texttt{scf.parallel}
is independent of all others, we can bind all the encompassed operations
to unique hardware resources (DSPs, LUTs, and FFs). Thus, we can infer
peak resource usage by computing the maximum cardinality of the cartesian
product of the iteration spaces of the parallel iteration variables
over all such parallel loops. For example, the convolution in Listing
\ref{lis:Single-filter-convolution-2} would bind to 
\begin{multline*}
K=\left|\left\{ \texttt{\%iv1}=\texttt{\%c0}+\texttt{\%c1}\times\mathbb{N}\,\wedge\,\texttt{\%iv1}<b\right\} \right|\times\\
\left|\left\{ \texttt{\%iv2}=\texttt{\%c0}+\texttt{\%c1}\times\mathbb{N}\,\wedge\,\texttt{\%iv2}<c_{out}\right\} \right|\times\\
\left|\left\{ \texttt{\%iv3}=\texttt{\%c0}+\texttt{\%c1}\times\mathbb{N}\,\wedge\,\texttt{\%iv3}<h\right\} \right|\times\\
\left|\left\{ \texttt{\%iv4}=\texttt{\%c0}+\texttt{\%c1}\times\mathbb{N}\,\wedge\,\texttt{\%iv4}<w\right\} \right|
\end{multline*}
collections of resources (i.e., $2K$ DSPs if \texttt{mul}, \texttt{add}
bind to 1 DSP each), where $\texttt{\%c1}\times\mathbb{N}$ represents
all multiples of $\texttt{\%c1}$. Note, DNN hyperparameters such
as striding and padding make this a non-trivial calculation. Once
peak usage is determined, successive instances \texttt{scf.parallel}
loop bodies evaluated at the same concrete values of (\texttt{\%iv1,
\%iv2, \%iv3, \%iv4}) are bound to the same resources. For DNN operations
that lower to sequential loop nests (e.g., \texttt{sum}, \texttt{max},or
\texttt{prod}), we fully unroll the loops and use a reduction tree
approach. 

In addition to enabling us to perform binding, as discussed in Section
\ref{subsec:High-level-synthesis}, a loop-nest representation enables
us to effectively perform data-flow analysis and schedule the encompassed
arithmetic operations (see the next Section regarding scheduling),
given that we can first unroll the loops. As also discussed in Section
\ref{subsec:High-level-synthesis}, the formally correct approach
to unrolling a loop nest is prohibitively expensive in terms of runtime.
Indeed, for \texttt{BraggNN}, with respect to producing a RTL representation
achieving latency within 1000$\times$ of the target latency (i.e.,
$<$1 ms), performing this unrolling in a reasonable amount of time
(e.g., $<$12 hours) was extremely challenging. To overcome this runtime
cost during the space exploration phase of the design process, the
approach that \texttt{BraggHLS} takes is to implement a \emph{symbolic
interpreter} for these loop-nest representations. Specifically, \texttt{BraggHLS}
translates \texttt{scf} dialect MLIR (such as in Listing \ref{lis:Single-filter-convolution-2})
into legal python code (such as in Listing \ref{lis:Single-filter-convolution})
and then, after applying some transformations on the resulting python
abstract syntax tree (AST), executes that python code. 

In executing the python code, the \texttt{BraggHLS} interpreter effectively
unrolls loops by executing them under a redefined set semantics, while
enforcing SSA. That is to say, for a loop whose body has repeated
assignments to the same SSA value (ostensibly violating SSA), we execute
the loop and instantiate unique identifiers for the result of each
operation. The interpreter then partially evaluates where possible,
such as in functions of iteration variables, such as \mintinline{mupad}!%3 = arith.addi %iv3, %iv6!.
This enables \texttt{BraggHLS} to concretely determine array index
operands of all stores and loads, such as \mintinline{mupad}!memref.load %input[%iv1,%iv5,%iv3,%3,%4]!. 

Note, we do not evaluate values corresponding to floating point arithmetic,
as they represent true evaluation of the DNN; our interpreter represents
the operands of such operations as symbols and merely records the
arithmetic operations performed on them. This enables us to both unroll
the loop and track data-flow through arithmetic operations (see Section
\ref{lis:Single-filter-convolution-1}). Finally, the interpreter
reinterprets \texttt{memref}s as \emph{geometric symbol tables} (i.e.,
symbol tables indexed by array indices rather than identifiers/names)
and stores and loads as assignments/reads to/from those symbol tables.
Such semantics, in combination with fully evaluated array indices,
enable \texttt{BraggHLS} to track the flow of data through arithmetic
operations and \texttt{memref}s, and thus perform store-load forwarding. 

\subsection{Scheduling}

Recall that one of the critical functions which HLS fulfills is the
scheduling of operations during each clock cycle, in such a way that
they preserve the data-flow graph of a DNN; that schedule then informs
the construction of a corresponding FSM. As already mentioned, scheduling
arbitrary DNNs involves formulating and solving an ILP. In the resource-unconstrained
case, due to the precedence relations induced by data-flow, the constraint
matrix of the associated ILP is \emph{totally unimodular matrix} and
the feasible region of the problem is an integral polyhedron. Thus,
in such cases, the scheduling problem can be solved optimally in polynomial
time with a LP solver \cite{tuprints9272}. 

In the resource constrained case it is possible to transform resource
constraints into precedence constraints as well, by picking a particular
(possible heuristic) linear ordering on the resource-constrained operations.
This transformation partitions resource constrained operations into
distinct clock cycles, thereby guaranteeing sufficient resources are
available for all operations scheduled within the same clock cycle
\cite{10.1145/3174243.3174268}. \texttt{BraggHLS} uses the inherent
parallelism of the loop-nest representation to decide a linear ordering
on resource constrained operations: since successive instances of
\texttt{scf.parallel} loop bodies evaluated at the same concrete values
of the iteration variables are bound to the same resources, all resource
constrained operations common to both instances are ordered. Note,
since each loop nest corresponds to a DNN operation, ordering of\texttt{
scf.parallel} loop nests is determined by the higher-level structure
of the DNN. For DNN operations that lower to reduction trees, we use
As-Late-As-Possible scheduling \cite{baruch1996scheduling} amongst
the subtrees.

\subsection{Floating point arithmetic implementations}

We use the FloPoCo \cite{8877424} hardware generator to generate
pipelined implementations of the standard floating point arithmetic
operations (\texttt{mul}, \texttt{div}, \texttt{add}, \texttt{sub},
\texttt{sqrt}) at various precisions. In addition, we implement a
few generic (parameterized by bit width) operators in order to support
a range of DNN operations:
\begin{itemize}
\item Two-operand maximum (\texttt{max});
\item Negation (\texttt{neg});
\item Rectified linear units (\texttt{relu}), which could, alternatively,
be represented as $\text{relu}\left(x\right):=\max(0,x)$.
\end{itemize}
Transcendental functions, such as \texttt{exp}, are implemented using
a Taylor series expansion to $k$th order (where $k$ is determined
on a case by case basis). Note, FloPoCo's floating point format differs
slightly from IEEE754, foregoing subnormals and differently encoding
zeroes, infinities and NaNs, for the benefit of reduced complexity. 

\subsection{AST transformations and behavioral verification\label{subsec:AST-transformations-and}}

In addition to unrolling loop-nests, \texttt{BraggHLS} performs some
simple AST transformations on the python generated from \texttt{scf}
dialect:
\begin{enumerate}
\item \textbf{Hoist globals}: all DNN tensors which are fixed (i.e., weights)
are moved out of the body of the python \footnote{\texttt{BraggHLS} translates the MLIR \texttt{module} corresponding
to the DNN into a single python function in order to simplify analysis
and interpretation.} and into the parameter list, for the purpose of ultimately exposing
them at the RTL module interface; 
\item \textbf{Remove }\texttt{\textbf{if}}\textbf{ expressions}: the \texttt{scf}
dialect represents \texttt{relu} operations as ternary conditionals,
which translate to python ternaries, thus introducing unnecessary
control flow;
\item \textbf{Remove MACs}: sequences of \texttt{load}-\texttt{multiply}-\texttt{add}-\texttt{store}
are very common in DNN implementations, thus we schedule such sequences
jointly (this transformation coalesces such sequences into a single
\texttt{FMAC});
\item \textbf{Reduce }\texttt{\textbf{for}}\textbf{s}: this transformation
implements the reduction tree structure for non-parallelizable loop-nests
mentioned in Section \ref{subsec:Symbolic-execution-for}.
\end{enumerate}
These transformations on the python AST are simple (implemented with
procedural pattern matching), extensible, and efficient (marginal
runtime cost) because they are unverified: no effort is made to verify
their formal correctness. Thus, \texttt{BraggHLS} trades formal correctness
for development time performance. This tradeoff enables quick design
space iteration, which for example, enabled us to achieve very low
latency implementations for \texttt{BraggNN} (see Section \ref{sec:BraggNN-case-study}).

As a substitute for formal verification, \texttt{BraggHLS} supports
behavioral verification. Specifically, \texttt{BraggHLS}, can generate
testbenches for all synthesized RTL. The test vectors for these testbenches
are generated by evaluating the transformed representation (i.e.,
compiling and executing the python AST) on randomly generated inputs
but with floating point operations now evaluated using functional
models of the corresponding FloPoCo operators\footnote{FloPoCo provides these functional models as C++ implementations -
we simply add python bindings to these C++ implementations.}.The testbenches can then be run using any IEEE 1364 compliant simulator.
For example, we run a battery of such testbenches (corresponding to
various DNN operation types), using \texttt{cocotb} \cite{rosser2018cocotb}
and \texttt{iverilog} \cite{williamsicarus}, as a part of our continuous
integration process\footnote{\url{https://github.com/makslevental/bragghls/actions}}.

\section{Evaluation\label{sec:Evaluation}}

asdasd
\begin{figure*}[tbh]
\begin{centering}
\subfloat[\texttt{addmm}]{\centering{}\includegraphics[width=1\columnwidth]{../data/addmm}\label{2dlattice-1-1-2}}\subfloat[\texttt{braggnn}]{\centering{}\includegraphics[width=1\columnwidth]{../data/braggnn}\label{2dlattice-1-2-2}}
\par\end{centering}
\medskip{}

\centering{}\label{2dlattice-1-3}\subfloat[\texttt{conv}]{\centering{}\includegraphics[width=1\columnwidth]{../data/conv}\label{2dlattice-1-1-1-1}}\subfloat[\texttt{matmul}]{\centering{}\includegraphics[width=1\columnwidth]{../data/matmul}\label{2dlattice-1-2-1-1}}\caption{Resource usage and latency vs. unroll factor of various DNN modules.}
\end{figure*}
\begin{figure*}[tbh]
\centering{}\includegraphics[width=1\columnwidth]{../data/elapsed_time}\caption{Runtime of Vitis HLS vs. unroll factor.}
\end{figure*}


\section{\texttt{BraggNN} case study\label{sec:BraggNN-case-study}}

High-energy diffraction microscopy techniques can provide non-destructive
characterization for a broad class of single-crystal and polycrystalline
materials. The critical steps in a typical HEDM experiment involve
an analysis to determine precise Bragg diffraction peak characteristics
(and reconstruction of material grain information from the peak characteristics).
Peak characteristics are typically computed by fitting the peaks to
a probability distribution, e.g., Gaussian, Lorentzian, Voigt, or
Pseudo-Voigt. As already mentioned (in Section \ref{sec:Introduction})
the experiments can have collection rates of 80 GB/s. The data rates,
though more modest than those observed at the LHC, combined with the
runtime of the fitting procedure, merit exploring the same low latency
approach in order to enable experimental modalities that depend on
measurement-based feedback (i.e., experiment steering). 
\begin{listing}
\begin{minted}[fontsize={\footnotesize},escapeinside={||},mathescape=true]{python}
BraggNN(
  (cnn_layers_1): Conv2d(|$k \times 16 $|, kernel=3, stride=1)
  (nlb): NLB(
    (theta_layer): Conv2d(|$k \times 16 $|, |$k \times 8 $|, kernel=1, stride=1)
    (phi_layer): Conv2d(|$k \times 16 $|, |$k \times 8 $|, kernel=1, stride=1)
    (g_layer): Conv2d(|$k \times 16 $|, |$k \times 8 $|, kernel=1, stride=1)
    (out_cnn): Conv2d(|$k \times 8 $|, |$k \times 16 $|, kernel=1, stride=1)
    (soft): Softmax()
  )
  (cnn_layers_2): Sequential(
    (0): ReLU()
    (1): Conv2d(|$k \times 16 $|, |$k \times 8 $|, kernel=3, stride=1)
    (2): ReLU()
    (3): Conv2d(|$k \times 8 $|, |$k \times 2 $|, kernel=3, stride=1)
    (4): ReLU()
  )
  (dense_layers): Sequential(
    (0): Linear(in_features=|$k \times 50$|, out_features=|$k \times 16 $|)
    (1): ReLU()
    (2): Linear(in_features=|$k \times 16 $|, out_features=|$k \times 8 $|)
    (3): ReLU()
    (4): Linear(in_features=|$k \times 8 $|, out_features=|$k \times 4 $|)
    (5): ReLU()
    (6): Linear(in_features=|$k \times 4 $|, out_features=2)
    (7): ReLU()
  )
)
\end{minted}
\caption{\texttt{BraggNN} for $k=1,\dots,4$.\label{lis:Single-filter-convolution-3}}
\end{listing}

\texttt{BraggNN} \cite{Liu:fs5198}, a DNN aimed at efficiently characterizing
Bragg diffraction peaks, achieves a batched latency of 22 \textmu s/sample.
This is a large speedup over the classical pseudo-Voigt peak fitting
methods, but still falls far short of the 1 \textmu s/sample target
for handling the 1 MHz sampling rates. In addition, the current implementation
of \texttt{BraggNN}, deployed to either a data-center class GPU such
as a NVIDIA V100, or even a workstation class GPU such as a NVIDIA
RTX 2080Ti, has no practicable means to being deployed at the edge,
i.e., adjacent or proximal to the high energy microscopy equipment.
We applied \texttt{BraggHLS} to the PyTorch representation of \texttt{BraggNN}\emph{
}(see Listing \ref{lis:Single-filter-convolution-3}) and achieve
a RTL implementation which synthesizes to a 1,238 interval design
that places, routes, and meets timing closure for a clock period of
10 ns (for a Xilinx Alveo U280). The design consists of a three stage
pipeline with long stage measuring 480 intervals. Thus, the throughput
of the implementation is 4.8 \textmu s/sample. See Table \ref{tab:Resource-usage-for}
for a summary of the resource usage of the implementation.
\begin{table*}[tbh]
\caption{Resource usage for \texttt{BraggNN} with $k=1$ and $\left(5,3\right)$-precision
FloPoCo \label{tab:Resource-usage-for}}

\begin{centering}
\begin{tabular}{lllllll}
\toprule 
Site Type & SLR0 & SLR1 & SLR2 & SLR0 \% & SLR1 \% & SLR2 \%\tabularnewline
\midrule 
CLB & 5047 & 52648 & 53900 & 9.18 & 97.50 & 99.81\tabularnewline
\midrule 
\quad{}CLBL & 2773 & 28613 & 29227 & 9.47 & 97.72 & 99.82\tabularnewline
\midrule 
\quad{}CLBM & 2274 & 24035 & 24673 & 8.86 & 97.23 & 99.81\tabularnewline
\midrule 
CLB LUTs & 19797 & 263733 & 311794 & 4.50 & 61.05 & 72.17\tabularnewline
\midrule 
\quad{}LUT as Logic & 19797 & 263733 & 311794 & 4.50 & 61.05 & 72.17\tabularnewline
\midrule 
\quad{}\quad{}using O5 output only & 277 & 3944 & 4304 & 0.06 & 0.91 & 1.00\tabularnewline
\midrule 
\quad{}\quad{}using O6 output only & 17176 & 202564 & 266733 & 3.91 & 46.89 & 61.74\tabularnewline
\midrule 
\quad{}\quad{}using O5 and O6 & 2344 & 57225 & 40757 & 0.53 & 13.25 & 9.43\tabularnewline
\midrule 
\quad{}LUT as Memory & 0 & 0 & 0 & 0.00 & 0.00 & 0.00\tabularnewline
\midrule 
\quad{}\quad{}LUT as Distributed RAM & 0 & 0 & 0 & 0.00 & 0.00 & 0.00\tabularnewline
\midrule 
\quad{}\quad{}LUT as Shift Register & 0 & 0 & 0 & 0.00 & 0.00 & 0.00\tabularnewline
\midrule 
CLB Registers & 12527 & 286226 & 339820 & 1.42 & 33.13 & 39.33\tabularnewline
\midrule 
CARRY8 & 244 & 5184 & 5184 & 0.44 & 9.60 & 9.60\tabularnewline
\midrule 
Block RAM Tile & 0 & 0 & 0 & 0.00 & 0.00 & 0.00\tabularnewline
\midrule 
\quad{}\quad{}RAMB36/FIFO & 0 & 0 & 0 & 0.00 & 0.00 & 0.00\tabularnewline
\midrule 
\quad{}\quad{}RAMB18 & 0 & 0 & 0 & 0.00 & 0.00 & 0.00\tabularnewline
\midrule 
URAM & 0 & 0 & 0 & 0.00 & 0.00 & 0.00\tabularnewline
\midrule 
DSPs & 0 & 0 & 0 & 0.00 & 0.00 & 0.00\tabularnewline
\midrule 
Unique Control Sets & 189 & 2641 & 3179 & 0.17 & 2.45 & 2.94\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}
\bigskip{}

\begin{centering}
\caption{Super long line usage across super logic regions for \texttt{BraggNN}\label{tab:Resource-usage-for-1}}
\par\end{centering}
\centering{}%
\begin{tabular}{lllll}
\toprule 
~ & Used & Fixed & Available & Util \%\tabularnewline
\midrule 
SLR2 $\leftrightarrow$ SLR1 & 21366 & ~ & 23040 & 92.73\tabularnewline
\midrule 
SLR1 $\rightarrow$ SLR2 & 2 & ~ & ~ & < 0.01\tabularnewline
\midrule 
SLR2 $\rightarrow$ SLR1 & 21364 & ~ & ~ & 92.73\tabularnewline
\midrule 
SLR1 $\leftrightarrow$ SLR0 & 3904 & ~ & 23040 & 16.94\tabularnewline
\midrule 
SLR0 $\rightarrow$ SLR1 & 2 & ~ & ~ & < 0.01\tabularnewline
\midrule 
SLR1 $\rightarrow$ SLR0 & 3902 & ~ & ~ & 16.94\tabularnewline
\midrule 
Total SLLs Used & 25270 & ~ & ~ & ~\tabularnewline
\bottomrule
\end{tabular}
\end{table*}

The most challenging aspect of implementing \texttt{BraggNN} was minimizing
latency while satisfying compute resource constraints (LUTs, DSPs,
BRAMs) and meeting routing ``closure'', i.e., not exceeding available
routing resources and avoiding congestion. Two design choices made
for the purposes of reducing resource consumption was reducing the
precision used for the floating point operations. We reduced the precision
from IEEE half precision (5 bits for the exponent and 11 bits for
the mantissa) to FloPoCo $\left(5,4\right)$-precision (5 bits for
the exponent and 4 bits for the mantissa). This was justified by an
examination of the distribution of the weights of the fully trained
\texttt{BraggNN} (see figure \ref{fig:BraggHLS-framework-overview.-1}).
\begin{figure}[tbh]
\centering{}\includegraphics[width=1\columnwidth]{../figures/exp_hist}\caption{\texttt{BraggHLS} weights exponent distribution (placeholder).\label{fig:BraggHLS-framework-overview.-1}}
\end{figure}
 Reducing the precision enabled us to eliminate BRAMs from the design
as well, since, at the lower precision, all weights could be represented
as registered constants. The reduced precision also drives the Vivado
synthesizer to infer implementations of the floating point operations
that make no use of DSPs; this was not intentional but seemingly cannot
be altereted. Most likely this is due to the fact that DSP48 hardware
block includes a 18-bit by 25-bit signed multiplier and a 48-bit adder
\cite{guideultrascale}, neither of which neatly divide the bit width
of FloPoCo $\left(5,4\right)$-precision cores\footnote{The actual bit width for FloPoCo $\left(5,4\right)$-precision is
12 bits: 1 extra bit is needed for the sign and 2 bits are needed
for FloPoCo's handling of exceptional conditions.}.

Achieving routing closure was very difficult due to the nature of
the Xilinx's UltraScale architecture, of which the Alveo U280 is an
instance. The UltraScale architecture achieves its scale through ``Stacked
Silicon Interconnect'' (SSI) technology \cite{leibson2013xilinx},
which amounts to multiple distinct FPGA dies, called Super Logic Regions
(SLRs), on the same chip, connected by interposers. Adjacent SLRs
communicate with each other using a limited set of Super Long Lines
(SLLs). These SLLs determine the maximum bus width that span two SLRs.
On the Alveo U280 there are exactly 23,040 SLLs available between
adjacent SLRs and at $\left(5,4\right)$-precision \texttt{BraggNN}
needs 23,328 SLLs between SLR2 and SLR1\footnote{We route the output of \texttt{cnn\_layers\_1} ($1\times16\times9\times9\times12$
wires) as well as the output of $\texttt{soft(theta\_layer}\times\texttt{phi\_layer)}\times\texttt{g\_layer}$
($1\times8\times9\times9\times12$ wires) from SLR2 to SLR1.}. Thus, we further reduce the precision to $\left(5,3\right)$. Finally,
since multiple dies constitute independent clock domains, the SLLs
that cross SLRS are sensitive hold time violations due to the higher
multi-die variability \cite{rapidwright}. This multi-die variability
leads to high congestion if not addressed. Thus, routing across them
needs to handled manually using placement and routing constraints
for logic in each SLR and the addition of so-called ``launch'' and
``latch'' registers in each SLR. See figure \ref{fig:BraggHLS-framework-overview.-2}
for an illustration on the effect of using launch and latch registers
as well as placement and routing constraints.
\begin{figure*}[tbh]
\centering{}\subfloat[\texttt{BraggNN} fails to achieve routing closure without placement
and routing constraints and launch and latch registers.\label{fig:BraggHLS-framework-overview.-2-1}]{\centering{}\includegraphics[width=1\columnwidth]{nopblocks}}\hfill{}\subfloat[\texttt{BraggNN} achieves routing closure with use of per SLR placement
and routing constraints (\texttt{pblock\_1}, \texttt{pblock\_2}, \texttt{pblock\_3})
and launch and latch registers (not highlighted).\label{fig:BraggHLS-framework-overview.-2-1-1}]{\centering{}\includegraphics[width=1\columnwidth]{withpblocks}}\caption{Congestion maps for \texttt{BraggNN} on a Xilinx Alveo U280. \textcolor{magenta}{Magenta}
indicates area of high congestion.\label{fig:BraggHLS-framework-overview.-2}}
\end{figure*}


\section{Conclusion\label{sec:Conclusion}}

287 + 471 + 480

\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}
