@misc{memento:rfc,
	author = {Herbert {Van de Sompel} and
			Michael L. Nelson and
			Robert Sanderson},
	title = {{HTTP Framework for Time-Based Access to Resource States -- Memento, Internet RFC 7089}},
	year = {2013},
	howpublished = {\url{https://tools.ietf.org/html/rfc7089}},
}


@InProceedings{pmlr-v42-glig14,
	title = {Real-time data analysis at the LHC: present and future},
	author = {Gligorov, Vladimir},
	booktitle = {Proceedings of the NIPS 2014 Workshop on High-energy Physics and Machine Learning},
	pages = {1--18},
	year = {2015},
	editor = {Cowan, Glen and Germain, Cecile and Guyon, Isabelle and Kegl, Balazs and Rousseau, David},
	volume = {42},
	series = {Proceedings of Machine Learning Research},
	address = {Montreal, Canada},
	month = {13 Dec},
	publisher = {PMLR},
	pdf = {http://proceedings.mlr.press/v42/glig14.pdf},
	url = {https://proceedings.mlr.press/v42/glig14.html},
	abstract = {The Large Hadron Collider (LHC), which collides protons at an energy of 14 TeV, produces hundreds of exabytes of data per year, making it one of the largest sources of data in the world today. At present it is not possible to even transfer most of this data from the four main particle detectors at the LHC to “offline” data facilities, much less to permanently store it for future processing. For this reason the LHC detectors are equipped with real-time analysis systems, called triggers, which process this volume of data and select the most interesting proton-proton (pp) collisions. The LHC experiment triggers reduce the data produced by the LHC by between 1/1000 and 1/100000, to tens of petabytes per year, allowing its economical storage and further analysis. The bulk of the data-reduction is performed by custom electronics which ignores most of the data in its decision making, and is therefore unable to exploit the most powerful known data analysis strategies. I cover the present status of real-time data analysis at the LHC, before explaining why the future upgrades of the LHC experiments will increase the volume of data which can be sent off the detector and into off-the-shelf data processing facilities (such as CPU or GPU farms) to tens of exabytes per year. This development will simultaneously enable a vast expansion of the physics programme of the LHC’s detectors, and make it mandatory to develop and implement a new generation of real-time multivariate analysis tools in order to fully exploit this new potential of the LHC. I explain what work is ongoing in this direction and motivate why more effort is needed in the coming years.}
}


@article{doi:10.1063/5.0006531,
	author = {Keunecke,Marius and M{\"o}ller,Christina and Schmitt,David and Nolte,Hendrik and Jansen,G. S. Matthijs and Reutzel,Marcel and Gutberlet,Marie and Halasi,Gyula and Steil,Daniel and Steil,Sabine and Mathias,Stefan},
	doi = {10.1063/5.0006531},
	eprint = {https://doi.org/10.1063/5.0006531},
	journal = {Review of Scientific Instruments},
	number = {6},
	pages = {063905},
	title = {Time-resolved momentum microscopy with a 1 MHz high-harmonic extreme ultraviolet beamline},
	url = {https://doi.org/10.1063/5.0006531},
	volume = {91},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1063/5.0006531} }


@article{BORK2021100619,
	abstract = {The Advanced LIGO detectors are sophisticated opto-mechanical devices. At the core of their operation is feedback control. The Advanced LIGO project developed a custom digital control and data acquisition system to handle the unique needs of this new breed of astronomical detector. The advligortsis the software component of this system. This highly modular and extensible system has enabled the unprecedented performance of the LIGO instruments, and has been a vital component in the direct detection of gravitational waves.},
	author = {Rolf Bork and Jonathan Hanks and David Barker and Joseph Betzwieser and Jameson Rollins and Keith Thorne and Erik {von Reis}},
	doi = {https://doi.org/10.1016/j.softx.2020.100619},
	issn = {2352-7110},
	journal = {SoftwareX},
	keywords = {Real-time processing, Feedback control, Hardware control, Data acquisition},
	pages = {100619},
	title = {advligorts: The Advanced LIGO real-time digital control and data acquisition system},
	url = {https://www.sciencedirect.com/science/article/pii/S2352711020303320},
	volume = {13},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S2352711020303320},
	bdsk-url-2 = {https://doi.org/10.1016/j.softx.2020.100619} }

@article{Guest:2018yhq,
author = "Guest, Dan and Cranmer, Kyle and Whiteson, Daniel",
title = "Deep Learning and its Application to {LHC} Physics",
eprint = "1806.11484",
archivePrefix = "arXiv",
primaryClass = "hep-ex",
doi = "10.1146/annurev-nucl-101917-021019",
journal = "Ann. Rev. Nucl. Part. Sci.",
volume = "68",
pages = "161--181",
year = "2018"
}

@article{paszke2017automatic,
	title = {Automatic differentiation in {PyTorch}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017}
}

@misc{https://doi.org/10.48550/arxiv.2002.11054,
	doi = {10.48550/ARXIV.2002.11054},

	url = {https://arxiv.org/abs/2002.11054},

	author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},

	keywords = {Programming Languages (cs.PL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

	title = {MLIR: A Compiler Infrastructure for the End of Moore's Law},

	publisher = {arXiv},

	year = {2020},

	copyright = {Creative Commons Attribution 4.0 International}
}


@misc{torch-mlir,
	author = {Sean Silva and Anush Elangovan},
	title = {{Torch-MLIR}},
	year = {2021},
	howpublished = {\url{https://mlir.llvm.org/OpenMeetings/2021-10-07-The-Torch-MLIR-project.pdf}},
}

@misc{polyhedral-mlir,
	author = {Uday Bondhugula},
	title = {{Polyhedral Compilation Opportunities in MLIR}},
	year = {2020},
	howpublished = {\url{https://acohen.gitlabpages.inria.fr/impact/impact2020/slides/IMPACT_2020_keynote.pdf}},
}


@inbook{Zhang2008,
	abstract = {The rapid increase of complexity in System-on-a-Chip design urges the design community to raise the level of abstraction beyond RTL. Automated behavior-level and system-level synthesis are naturally identified as next steps to replace RTL synthesis and will greatly boost the adoption of electronic system-level (ESL) design. High-level executable specifications, such as C, C++, or SystemC, are also preferred for system-level verification and hardware/software co-design.},
	address = {Dordrecht},
	author = {Zhang, Zhiru and Fan, Yiping and Jiang, Wei and Han, Guoling and Yang, Changqi and Cong, Jason},
	booktitle = {High-Level Synthesis: From Algorithm to Digital Circuit},
	doi = {10.1007/978-1-4020-8588-8_6},
	editor = {Coussy, Philippe and Morawiec, Adam},
	isbn = {978-1-4020-8588-8},
	pages = {99--112},
	publisher = {Springer Netherlands},
	title = {AutoPilot: A Platform-Based ESL Synthesis System},
	url = {https://doi.org/10.1007/978-1-4020-8588-8_6},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1007/978-1-4020-8588-8_6} }

@article{10.1145/2514740,
	author = {Canis, Andrew and Choi, Jongsok and Aldham, Mark and Zhang, Victor and Kammoona, Ahmed and Czajkowski, Tomasz and Brown, Stephen D. and Anderson, Jason H.},
	title = {LegUp: An Open-Source High-Level Synthesis Tool for FPGA-Based Processor/Accelerator Systems},
	year = {2013},
	issue_date = {September 2013},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {13},
	number = {2},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/2514740},
	doi = {10.1145/2514740},
	abstract = {It is generally accepted that a custom hardware implementation of a set of computations will provide superior speed and energy efficiency relative to a software implementation. However, the cost and difficulty of hardware design is often prohibitive, and consequently, a software approach is used for most applications. In this article, we introduce a new high-level synthesis tool called LegUp that allows software techniques to be used for hardware design. LegUp accepts a standard C program as input and automatically compiles the program to a hybrid architecture containing an FPGA-based MIPS soft processor and custom hardware accelerators that communicate through a standard bus interface. In the hybrid processor/accelerator architecture, program segments that are unsuitable for hardware implementation can execute in software on the processor. LegUp can synthesize most of the C language to hardware, including fixed-sized multidimensional arrays, structs, global variables, and pointer arithmetic. Results show that the tool produces hardware solutions of comparable quality to a commercial high-level synthesis tool. We also give results demonstrating the ability of the tool to explore the hardware/software codesign space by varying the amount of a program that runs in software versus hardware. LegUp, along with a set of benchmark C programs, is open source and freely downloadable, providing a powerful platform that can be leveraged for new research on a wide range of high-level synthesis topics.},
	journal = {ACM Trans. Embed. Comput. Syst.},
	month = {sep},
	articleno = {24},
	numpages = {27},
	keywords = {power, performance, field-programmable gate arrays, FPGAs, synthesis, hardware/software codesign, High-level synthesis}
}

@INPROCEEDINGS{ferrandi2021bambu,

	author = {Ferrandi, Fabrizio and Castellana, Vito Giovanni

			and Curzel, Serena and Fezzardi, Pietro and Fiorito, Michele

			and Lattuada, Marco and Minutoli, Marco and Pilato, Christian

			and Tumeo, Antonino},

	booktitle = {2021 58th ACM/IEEE Design Automation Conference (DAC)},

	title = {Invited: Bambu: an Open-Source Research Framework for the

			High-Level Synthesis of Complex Applications},

	year = {2021},

	pages = {1327-1330},

	publisher = {{IEEE}},

}

@phdthesis{tuprints9272,
	title = {Advances in ILP-based Modulo Scheduling for High-Level Synthesis},
	year = {2019},
	language = {en},
	author = {Julian Oppermann},
	school = {Technische Universit{\"a}t},
	address = {Darmstadt},
	url = {http://tuprints.ulb.tu-darmstadt.de/9272/},
	abstract = {In today's heterogenous computing world, field-programmable gate arrays (FPGA) represent the energy-efficient alternative to generic processor cores and graphics accelerators. However, due to their radically different computing model, automatic design methods, such as high-level synthesis (HLS), are needed to harness their full power. HLS raises the abstraction level to behavioural descriptions of algorithms, thus freeing designers from dealing with tedious low-level concerns, and enabling a rapid exploration of different microarchitectures for the same input specification. In an HLS tool, scheduling is the most influential step for the performance of the generated accelerator. Specifically, modulo schedulers enable a pipelined execution, which is a key technique to speed up the computation by extracting more parallelism from the input description. In this thesis, we make a case for the use of integer linear programming (ILP) as a framework for modulo scheduling approaches. First, we argue that ILP-based modulo schedulers are practically usable in the HLS context. Secondly, we show that the ILP framework enables a novel approach for the automatic design of FPGA accelerators. We substantiate the first claim by proposing a new, flexible ILP formulation for the modulo scheduling problem, and evaluate it experimentally with a diverse set of realistic test instances. While solving an ILP may incur an exponential runtime in the worst case, we observe that simple countermeasures, such as setting a time limit, help to contain the practical impact of outlier instances. Furthermore, we present an algorithm to compress problems before the actual scheduling. An HLS-generated microarchitecture is comprised of operators, i.e. single-purpose functional units such as a floating-point multiplier. Usually, the allocation of operators is determined before scheduling, even though both problems are interdependent. To that end, we investigate an extension of the modulo scheduling problem that combines both concerns in a single model. Based on the extension, we present a novel multi-loop scheduling approach capable of finding the fastest microarchitecture that still fits on a given FPGA device - an optimisation problem that current commercial HLS tools cannot solve. This proves our second claim.}
}

@INPROCEEDINGS{1688836,  author = {Cong, J. and Zhiru Zhang},  booktitle = {2006 43rd ACM/IEEE Design Automation Conference},   title = {An efficient and versatile scheduling algorithm based on SDC formulation},   year = {2006},  volume = {},  number = {},  pages = {433-438},  doi = {10.1145/1146909.1147025} }

@INPROCEEDINGS{6546003,  author={Soni, Ritesh Kumar and Steiner, Neil and French, Matthew},  booktitle={2013 IEEE 21st Annual International Symposium on Field-Programmable Custom Computing Machines},   title={Open-Source Bitstream Generation},   year={2013},  volume={},  number={},  pages={105-112},  doi={10.1109/FCCM.2013.45}}

@inproceedings{wolf2013yosys,
  title={Yosys-a free Verilog synthesis suite},
  author={Wolf, Clifford and Glaser, Johann and Kepler, Johannes},
  booktitle={Proceedings of the 21st Austrian Workshop on Microelectronics (Austrochip)},
  year={2013}
}


@article{10.1145/2858788.2688521,
author = {Ashari, Arash and Tatikonda, Shirish and Boehm, Matthias and Reinwald, Berthold and Campbell, Keith and Keenleyside, John and Sadayappan, P.},
title = {On Optimizing Machine Learning Workloads via Kernel Fusion},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858788.2688521},
doi = {10.1145/2858788.2688521},
abstract = {},
month = {jan},
pages = {173-182},
numpages = {10},
keywords = {Dense, GPU, Machine Learning, Sparse, Fused Kernel}
}


@article{osti_1574050,
title = {Hierarchical Roofline analysis for GPUs: Accelerating performance optimization for the NERSC‐9 Perlmutter system},
author = {Yang, Charlene and Kurth, Thorsten and Williams, Samuel},
abstractNote = {},
doi = {10.1002/cpe.5547},
journal = {Concurrency and Computation. Practice and Experience},
number = 20,
volume = 32,
place = {United Kingdom},
year = {2019},
month = {11}
}


@book{thomas1971catalogue,
  title={A catalogue of optimizing transformations},
  author={Thomas J. Watson IBM Research Center. Research Division and Allen, FE and Cocke, J},
  year={1971}
}


@book{sanders2019sequential,
  title={Sequential and Parallel Algorithms and Data Structures},
  author={Sanders, Peter and Mehlhorn, Kurt and Dietzfelbinger, Martin and Dementiev, Roman},
  year={2019},
  publisher={Springer},
  chapter={9},
  section={9.4},
}

  @misc{ enwiki:1081681080,
    author = "{Wikipedia contributors}",
    title = "Fast inverse square root --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2022",
    howpublished = "\url{https://en.wikipedia.org/wiki/Fast_inverse_square_root}",
    note = "[Online; accessed 3-May-2022]"
  }


@inproceedings{10.1007/978-0-387-72258-0_14,
	address = {Boston, MA},
	author = {Middendorf, Lars and M{\"u}hlbauer, Felix and Umlauf, Georg and Bobda, Christophe},
	booktitle = {Embedded System Design: Topics, Techniques and Trends},
	editor = {Rettberg, Achim and Zanella, Mauro C. and D{\"o}mer, Rainer and Gerstlauer, Andreas and Rammig, Franz J.},
	isbn = {978-0-387-72258-0},
	pages = {155--164},
	publisher = {Springer US},
	title = {Embedded Vertex Shader in FPGA},
	year = {2007}}

@INPROCEEDINGS{8877424,  author={de Dinechin, Florent},  booktitle={2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)},   title={Reflections on 10 Years of FloPoCo},   year={2019},  volume={},  number={},  pages={187-189},  doi={10.1109/ARITH.2019.00042}}


@article{ye2021scalehls,
  title={ScaleHLS: A New Scalable High-Level Synthesis Framework on Multi-Level Intermediate Representation},
  author={Ye, Hanchen and Hao, Cong and Cheng, Jianyi and Jeong, Hyunmin and Huang, Jack and Neuendorffer, Stephen and Chen, Deming},
  journal={arXiv preprint arXiv:2107.11673},
  year={2021}
}

@INPROCEEDINGS{9516615,  author={Zhang, Jeff Jun and Bohm Agostini, Nicolas and Song, Shihao and Tan, Cheng and Limaye, Ankur and Amatya, Vinay and Manzano, Joseph and Minutoli, Marco and Castellana, Vito Giovanni and Tumeo, Antonino and Wei, Gu-Yeon and Brooks, David},  booktitle={2021 IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP)},   title={Towards Automatic and Agile AI/ML Accelerator Design with End-to-End Synthesis},   year={2021},  volume={},  number={},  pages={218-225},  doi={10.1109/ASAP52443.2021.00040}}

@inproceedings{nikhil2004bluespec,
  title={Bluespec System Verilog: efficient, correct RTL from high level specifications},
  author={Nikhil, Rishiyur},
  booktitle={Proceedings. Second ACM and IEEE International Conference on Formal Methods and Models for Co-Design, 2004. MEMOCODE'04.},
  pages={69--70},
  year={2004},
  organization={IEEE}
}


@article{reconfigfpga,
	abstract = {Reconfigurable computing is a potential paradigm which has been effectively performing mostly in the developments of devices likely Field Programmable Gate Arrays (FPGAs). This paper illustrates the reconfigurable architecture of FPGA and its types. Most widely used high-speed computation fabrics utilized in reconfigurable computing are FPGAs. This paper demonstrates the architectures used in reconfigurable computing and shows the various advantages of using reconfigurable computing design over conventional Application-Specific Integrated Circuits for achieving high level of performance for a desired application. The survey deals with the architecture of FPGAs and their types in detail. This paper also explains the highlights and challenges of fine-grained and coarse-grained architectures. FPGAs have supported partial reconfiguration over the few years. This survey also includes the partial reconfiguration techniques and the various applications of reconfigurability.},
	author = {Babu, Praveenkumar and Parthasarathy, Eswaran},
	date = {2021/02/01},
	date-added = {2022-05-03 17:40:19 -0500},
	date-modified = {2022-05-03 17:40:19 -0500},
	doi = {10.1007/s40031-020-00508-y},
	id = {Babu2021},
	isbn = {2250-2114},
	journal = {Journal of The Institution of Engineers (India): Series B},
	number = {1},
	pages = {143--156},
	title = {Reconfigurable FPGA Architectures: A Survey and Applications},
	url = {https://doi.org/10.1007/s40031-020-00508-y},
	volume = {102},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s40031-020-00508-y}}

@inproceedings{10.1145/3211346.3211348,
author = {Roesch, Jared and Lyubomirsky, Steven and Weber, Logan and Pollock, Josh and Kirisame, Marisa and Chen, Tianqi and Tatlock, Zachary},
title = {Relay: A New IR for Machine Learning Frameworks},
year = {2018},
isbn = {9781450358347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211346.3211348},
doi = {10.1145/3211346.3211348},
abstract = {Machine learning powers diverse services in industry including search, translation, recommendation systems, and security. The scale and importance of these models require that they be efficient, expressive, and portable across an array of heterogeneous hardware devices. These constraints are often at odds; in order to better accommodate them we propose a new high-level intermediate representation (IR) called Relay. Relay is being designed as a purely-functional, statically-typed language with the goal of balancing efficient compilation, expressiveness, and portability. We discuss the goals of Relay and highlight its important design constraints. Our prototype is part of the open source NNVM compiler framework, which powers Amazon's deep learning framework MxNet.},
booktitle = {Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {58–68},
numpages = {11},
keywords = {intermediate representation, compilers, machine learning, differentiable programming},
location = {Philadelphia, PA, USA},
series = {MAPL 2018}
}

@article{vtr,
author = {Murray, Kevin and Petelin, Oleg and Zhong, Sheng and Wang, Jia and Eldafrawy, Mohamed and Legault, Jean-Philippe and Sha, Eugene and Graham, Aaron and Wu, Jean and Walker, Matthew and Zeng, Hanqing and Patros, Panos and Luu, Jason and Kent, Kenneth and Betz, Vaughn},
year = {2020},
month = {05},
pages = {1-55},
title = {VTR 8: High-performance CAD and Customizable FPGA Architecture Modelling},
volume = {13},
journal = {ACM Transactions on Reconfigurable Technology and Systems},
doi = {10.1145/3388617}
}

@inproceedings{mcmullin2022square,
  title={The Square Kilometre Array project update},
  author={McMullin, J and Diamond, P and Caiazzo, M and Casson, A and Cheetham, T and Dewdney, P and Laing, R and Lewis, B and Schinckel, A and Stringhetti, L and others},
  booktitle={Ground-based and Airborne Telescopes IX},
  volume={12182},
  pages={263--271},
  year={2022},
  organization={SPIE}
}

@article{grainge2017square,
  title={Square Kilometre Array: The radio telescope of the XXI century},
  author={Grainge, Keith and Alachkar, Bassem and Amy, Shaun and Barbosa, Domingos and Bommineni, Murali and Boven, Paul and Braddock, Ralph and Davis, John and Diwakar, Praveen and Francis, Vishal and others},
  journal={Astronomy reports},
  volume={61},
  number={4},
  pages={288--296},
  year={2017},
  publisher={Springer}
}

@article{Hammer_2021,
	doi = {10.1088/1748-0221/16/01/p01025},
  
	url = {https://doi.org/10.1088%2F1748-0221%2F16%2F01%2Fp01025},
  
	year = 2021,
	month = {jan},
  
	publisher = {{IOP} Publishing},
  
	volume = {16},
  
	number = {01},
  
	pages = {P01025--P01025},
  
	author = {M. Hammer and K. Yoshii and A. Miceli},
  
	title = {Strategies for on-chip digital data compression for X-ray pixel detectors},
  
	journal = {Journal of Instrumentation}
}

@article{LHCB-FIGURE-2020-018,
      title         = "{Comparison of particle selection algorithms for the LHCb
                       Upgrade}",
      collaboration = "LHCb Collaboration",
      year          = "2020",
      url           = "https://cds.cern.ch/record/2746789",
}

@article{Gligorov_2013,
	doi = {10.1088/1748-0221/8/02/p02013},
  
	url = {https://doi.org/10.1088%2F1748-0221%2F8%2F02%2Fp02013},
  
	year = 2013,
	month = {feb},
  
	publisher = {{IOP} Publishing},
  
	volume = {8},
  
	number = {02},
  
	pages = {P02013--P02013},
  
	author = {V V Gligorov and M Williams},
  
	title = {Efficient, reliable and fast high-level triggering using a bonsai boosted decision tree},
  
	journal = {Journal of Instrumentation}
}

@article{aaij2020allen,
  title={Allen: A high-level trigger on GPUs for LHCb},
  author={Aaij, Roel and Albrecht, Johannes and Belous, M and Billoir, P and Boettcher, T and Brea Rodr{\'\i}guez, A and Vom Bruch, D and C{\'a}mpora P{\'e}rez, DH and Casais Vidal, A and Craik, DC and others},
  journal={Computing and Software for big Science},
  volume={4},
  number={1},
  pages={1--11},
  year={2020},
  publisher={Springer}
}

@article{alzubaidi2021review,
  title={Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions},
  author={Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J and Al-Dujaili, Ayad and Duan, Ye and Al-Shamma, Omran and Santamar{\'\i}a, Jos{\'e} and Fadhel, Mohammed A and Al-Amidie, Muthana and Farhan, Laith},
  journal={Journal of big Data},
  volume={8},
  number={1},
  pages={1--74},
  year={2021},
  publisher={Springer}
}

@misc{https://doi.org/10.48550/arxiv.1603.04467,
  doi = {10.48550/ARXIV.1603.04467},
  
  url = {https://arxiv.org/abs/1603.04467},
  
  author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1512.01274,
  doi = {10.48550/ARXIV.1512.01274},
  
  url = {https://arxiv.org/abs/1512.01274},
  
  author = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), Mathematical Software (cs.MS), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{liu2019deep,
  title={Deep learning accelerated light source experiments},
  author={Liu, Zhengchun and Bicer, Tekin and Kettimuthu, Rajkumar and Foster, Ian},
  booktitle={2019 IEEE/ACM Third Workshop on Deep Learning on Supercomputers (DLS)},
  pages={20--28},
  year={2019},
  organization={IEEE}
}

@article{liu2022exploring,
  title={Exploring physics of ferroelectric domain walls in real time: deep learning enabled scanning probe microscopy},
  author={Liu, Yongtao and Kelley, Kyle P and Funakubo, Hiroshi and Kalinin, Sergei V and Ziatdinov, Maxim},
  journal={Advanced Science},
  pages={2203957},
  year={2022},
  publisher={Wiley Online Library}
}

@inproceedings{patton2018167,
  title={167-pflops deep learning for electron microscopy: from learning physics to atomic manipulation},
  author={Patton, Robert M and Johnston, J Travis and Young, Steven R and Schuman, Catherine D and March, Don D and Potok, Thomas E and Rose, Derek C and Lim, Seung-Hwan and Karnowski, Thomas P and Ziatdinov, Maxim A and others},
  booktitle={SC18: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={638--648},
  year={2018},
  organization={IEEE}
}

@inproceedings{chen2018tvm,
  title={$\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={578--594},
  year={2018}
}

@misc{https://doi.org/10.48550/arxiv.1805.00907,
  doi = {10.48550/ARXIV.1805.00907},
  
  url = {https://arxiv.org/abs/1805.00907},
  
  author = {Rotem, Nadav and Fix, Jordan and Abdulrasool, Saleem and Catron, Garret and Deng, Summer and Dzhabarov, Roman and Gibson, Nick and Hegeman, James and Lele, Meghan and Levenstein, Roman and Montgomery, Jack and Maher, Bert and Nadathur, Satish and Olesen, Jakob and Park, Jongsoo and Rakhov, Artem and Smelyanskiy, Misha and Wang, Man},
  
  keywords = {Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Glow: Graph Lowering Compiler Techniques for Neural Networks},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{https://doi.org/10.48550/arxiv.1809.02697,
  doi = {10.48550/ARXIV.1809.02697},
  
  url = {https://arxiv.org/abs/1809.02697},
  
  author = {Liu, Yizhi and Wang, Yao and Yu, Ruofei and Li, Mu and Sharma, Vin and Wang, Yida},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Optimizing CNN Model Inference on CPUs},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE {9664259,
author = {S. Zheng and R. Chen and Y. Jin and A. Wei and B. Wu and X. Li and S. Yan and Y. Liang},
journal = {IEEE Transactions on Parallel and Distributed Systems},
title = {NeoFlow: A Flexible Framework for Enabling Efficient Compilation for High Performance DNN Training},
year = {2022},
volume = {33},
number = {11},
issn = {1558-2183},
pages = {3220-3232},
abstract = {Deep neural networks (DNNs) are increasingly deployed in various image recognition and natural language processing applications. The continuous demand for accuracy and high performance has led to innovations in DNN design and a proliferation of new operators. However, existing DNN training frameworks such as PyTorch and TensorFlow only support a limited range of operators and rely on hand-optimized libraries to provide efficient implementations for these operators. To evaluate novel neural networks with new operators, the programmers have to either replace the holistic new operators with existing operators or provide low-level implementations manually. Therefore, a critical requirement for DNN training frameworks is to provide high-performance implementations for the neural networks containing new operators automatically in the absence of efficient library support. In this article, we introduce NeoFlow, which is a flexible framework for enabling efficient compilation for high-performance DNN training. NeoFlow allows the programmers to directly write customized expressions as new operators to be mapped to graph representation and low-level implementations automatically, providing both high programming productivity and high performance. First, NeoFlow provides expression-based automatic differentiation to support customized model definitions with new operators. Then, NeoFlow proposes an efficient compilation system that partitions the neural network graph into subgraphs, explores optimized schedules, and generates high-performance libraries for subgraphs automatically. Finally, NeoFlow develops an efficient runtime system to combine the compilation and training as a whole by overlapping their execution. In the experiments, we examine the numerical accuracy and performance of NeoFlow. The results show that NeoFlow can achieve similar or even better performance at the operator and whole graph level for DNNs compared to deep learning frameworks. Especially, for novel networks training, the geometric mean speedups of NeoFlow to PyTorch, TensorFlow, and CuDNN are 3.16X, 2.43X, and 1.92X, respectively.},
keywords = {training;libraries;convolution;codes;deep learning;tensors;schedules},
doi = {10.1109/TPDS.2021.3138862},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {nov}
}


@inproceedings{maleki2011evaluation,
  title={An evaluation of vectorizing compilers},
  author={Maleki, Saeed and Gao, Yaoqing and Garzar, Maria J and Wong, Tommy and Padua, David A and others},
  booktitle={2011 International Conference on Parallel Architectures and Compilation Techniques},
  pages={372--382},
  year={2011},
  organization={IEEE}
}

@misc{https://doi.org/10.48550/arxiv.1604.06174,
  doi = {10.48550/ARXIV.1604.06174},
  
  url = {https://arxiv.org/abs/1604.06174},
  
  author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training Deep Nets with Sublinear Memory Cost},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Duarte_2018,
	doi = {10.1088/1748-0221/13/07/p07027},
  
	url = {https://doi.org/10.1088%2F1748-0221%2F13%2F07%2Fp07027},
  
	year = 2018,
	month = {jul},
  
	publisher = {{IOP} Publishing},
  
	volume = {13},
  
	number = {07},
  
	pages = {P07027--P07027},
  
	author = {J. Duarte and S. Han and P. Harris and S. Jindariani and E. Kreinar and B. Kreis and J. Ngadiuba and M. Pierini and R. Rivera and N. Tran and Z. Wu},
  
	title = {Fast inference of deep neural networks in {FPGAs} for particle physics},
  
	journal = {Journal of Instrumentation}
}

@ARTICLE{7368920,  author={Nane, Razvan and Sima, Vlad-Mihai and Pilato, Christian and Choi, Jongsok and Fort, Blair and Canis, Andrew and Chen, Yu Ting and Hsiao, Hsuan and Brown, Stephen and Ferrandi, Fabrizio and Anderson, Jason and Bertels, Koen},  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},   title={A Survey and Evaluation of FPGA High-Level Synthesis Tools},   year={2016},  volume={35},  number={10},  pages={1591-1604},  doi={10.1109/TCAD.2015.2513673}}


@misc{https://doi.org/10.48550/arxiv.2203.08402,
  doi = {10.48550/ARXIV.2203.08402},
  
  url = {https://arxiv.org/abs/2203.08402},
  
  author = {Hattori, Momoko and Kobayashi, Naoki and Sato, Ryosuke},
  
  keywords = {Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences, D.2.4},
  
  title = {Gradual Tensor Shape Checking},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
