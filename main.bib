@misc{memento:rfc,
	author = {Herbert {Van de Sompel} and
			Michael L. Nelson and
			Robert Sanderson},
	title = {{HTTP Framework for Time-Based Access to Resource States -- Memento, Internet RFC 7089}},
	year = {2013},
	howpublished = {\url{https://tools.ietf.org/html/rfc7089}},
}


@InProceedings{pmlr-v42-glig14,
	title = {Real-time data analysis at the LHC: present and future},
	author = {Gligorov, Vladimir},
	booktitle = {Proceedings of the NIPS 2014 Workshop on High-energy Physics and Machine Learning},
	pages = {1--18},
	year = {2015},
	editor = {Cowan, Glen and Germain, Cécile and Guyon, Isabelle and Kégl, Balázs and Rousseau, David},
	volume = {42},
	series = {Proceedings of Machine Learning Research},
	address = {Montreal, Canada},
	month = {13 Dec},
	publisher = {PMLR},
	pdf = {http://proceedings.mlr.press/v42/glig14.pdf},
	url = {https://proceedings.mlr.press/v42/glig14.html},
	abstract = {The Large Hadron Collider (LHC), which collides protons at an energy of 14 TeV, produces hundreds of exabytes of data per year, making it one of the largest sources of data in the world today. At present it is not possible to even transfer most of this data from the four main particle detectors at the LHC to “offline” data facilities, much less to permanently store it for future processing. For this reason the LHC detectors are equipped with real-time analysis systems, called triggers, which process this volume of data and select the most interesting proton-proton (pp) collisions. The LHC experiment triggers reduce the data produced by the LHC by between 1/1000 and 1/100000, to tens of petabytes per year, allowing its economical storage and further analysis. The bulk of the data-reduction is performed by custom electronics which ignores most of the data in its decision making, and is therefore unable to exploit the most powerful known data analysis strategies. I cover the present status of real-time data analysis at the LHC, before explaining why the future upgrades of the LHC experiments will increase the volume of data which can be sent off the detector and into off-the-shelf data processing facilities (such as CPU or GPU farms) to tens of exabytes per year. This development will simultaneously enable a vast expansion of the physics programme of the LHC’s detectors, and make it mandatory to develop and implement a new generation of real-time multivariate analysis tools in order to fully exploit this new potential of the LHC. I explain what work is ongoing in this direction and motivate why more effort is needed in the coming years.}
}


@article{doi:10.1063/5.0006531,
	author = {Keunecke,Marius and M{\"o}ller,Christina and Schmitt,David and Nolte,Hendrik and Jansen,G. S. Matthijs and Reutzel,Marcel and Gutberlet,Marie and Halasi,Gyula and Steil,Daniel and Steil,Sabine and Mathias,Stefan},
	doi = {10.1063/5.0006531},
	eprint = {https://doi.org/10.1063/5.0006531},
	journal = {Review of Scientific Instruments},
	number = {6},
	pages = {063905},
	title = {Time-resolved momentum microscopy with a 1 MHz high-harmonic extreme ultraviolet beamline},
	url = {https://doi.org/10.1063/5.0006531},
	volume = {91},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1063/5.0006531} }


@article{BORK2021100619,
	abstract = {The Advanced LIGO detectors are sophisticated opto-mechanical devices. At the core of their operation is feedback control. The Advanced LIGO project developed a custom digital control and data acquisition system to handle the unique needs of this new breed of astronomical detector. The advligortsis the software component of this system. This highly modular and extensible system has enabled the unprecedented performance of the LIGO instruments, and has been a vital component in the direct detection of gravitational waves.},
	author = {Rolf Bork and Jonathan Hanks and David Barker and Joseph Betzwieser and Jameson Rollins and Keith Thorne and Erik {von Reis}},
	doi = {https://doi.org/10.1016/j.softx.2020.100619},
	issn = {2352-7110},
	journal = {SoftwareX},
	keywords = {Real-time processing, Feedback control, Hardware control, Data acquisition},
	pages = {100619},
	title = {advligorts: The Advanced LIGO real-time digital control and data acquisition system},
	url = {https://www.sciencedirect.com/science/article/pii/S2352711020303320},
	volume = {13},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S2352711020303320},
	bdsk-url-2 = {https://doi.org/10.1016/j.softx.2020.100619} }

@article{Guest:2018yhq,
author = "Guest, Dan and Cranmer, Kyle and Whiteson, Daniel",
title = "{Deep Learning and its Application to LHC Physics}",
eprint = "1806.11484",
archivePrefix = "arXiv",
primaryClass = "hep-ex",
doi = "10.1146/annurev-nucl-101917-021019",
journal = "Ann. Rev. Nucl. Part. Sci.",
volume = "68",
pages = "161--181",
year = "2018"
}

@article{paszke2017automatic,
	title = {Automatic differentiation in pytorch},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017}
}

@misc{https://doi.org/10.48550/arxiv.2002.11054,
	doi = {10.48550/ARXIV.2002.11054},

	url = {https://arxiv.org/abs/2002.11054},

	author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},

	keywords = {Programming Languages (cs.PL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

	title = {MLIR: A Compiler Infrastructure for the End of Moore's Law},

	publisher = {arXiv},

	year = {2020},

	copyright = {Creative Commons Attribution 4.0 International}
}


@misc{torch-mlir,
	author = {Sean Silva and Anush Elangovan},
	title = {{Torch-MLIR}},
	year = {2021},
	howpublished = {\url{https://mlir.llvm.org/OpenMeetings/2021-10-07-The-Torch-MLIR-project.pdf}},
}

@misc{polyhedral-mlir,
	author = {Uday Bondhugula},
	title = {{Polyhedral Compilation Opportunities in MLIR}},
	year = {2020},
	howpublished = {\url{https://acohen.gitlabpages.inria.fr/impact/impact2020/slides/IMPACT_2020_keynote.pdf}},
}


@inbook{Zhang2008,
	abstract = {The rapid increase of complexity in System-on-a-Chip design urges the design community to raise the level of abstraction beyond RTL. Automated behavior-level and system-level synthesis are naturally identified as next steps to replace RTL synthesis and will greatly boost the adoption of electronic system-level (ESL) design. High-level executable specifications, such as C, C++, or SystemC, are also preferred for system-level verification and hardware/software co-design.},
	address = {Dordrecht},
	author = {Zhang, Zhiru and Fan, Yiping and Jiang, Wei and Han, Guoling and Yang, Changqi and Cong, Jason},
	booktitle = {High-Level Synthesis: From Algorithm to Digital Circuit},
	doi = {10.1007/978-1-4020-8588-8_6},
	editor = {Coussy, Philippe and Morawiec, Adam},
	isbn = {978-1-4020-8588-8},
	pages = {99--112},
	publisher = {Springer Netherlands},
	title = {AutoPilot: A Platform-Based ESL Synthesis System},
	url = {https://doi.org/10.1007/978-1-4020-8588-8_6},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1007/978-1-4020-8588-8_6} }

@article{10.1145/2514740,
	author = {Canis, Andrew and Choi, Jongsok and Aldham, Mark and Zhang, Victor and Kammoona, Ahmed and Czajkowski, Tomasz and Brown, Stephen D. and Anderson, Jason H.},
	title = {LegUp: An Open-Source High-Level Synthesis Tool for FPGA-Based Processor/Accelerator Systems},
	year = {2013},
	issue_date = {September 2013},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {13},
	number = {2},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/2514740},
	doi = {10.1145/2514740},
	abstract = {It is generally accepted that a custom hardware implementation of a set of computations will provide superior speed and energy efficiency relative to a software implementation. However, the cost and difficulty of hardware design is often prohibitive, and consequently, a software approach is used for most applications. In this article, we introduce a new high-level synthesis tool called LegUp that allows software techniques to be used for hardware design. LegUp accepts a standard C program as input and automatically compiles the program to a hybrid architecture containing an FPGA-based MIPS soft processor and custom hardware accelerators that communicate through a standard bus interface. In the hybrid processor/accelerator architecture, program segments that are unsuitable for hardware implementation can execute in software on the processor. LegUp can synthesize most of the C language to hardware, including fixed-sized multidimensional arrays, structs, global variables, and pointer arithmetic. Results show that the tool produces hardware solutions of comparable quality to a commercial high-level synthesis tool. We also give results demonstrating the ability of the tool to explore the hardware/software codesign space by varying the amount of a program that runs in software versus hardware. LegUp, along with a set of benchmark C programs, is open source and freely downloadable, providing a powerful platform that can be leveraged for new research on a wide range of high-level synthesis topics.},
	journal = {ACM Trans. Embed. Comput. Syst.},
	month = {sep},
	articleno = {24},
	numpages = {27},
	keywords = {power, performance, field-programmable gate arrays, FPGAs, synthesis, hardware/software codesign, High-level synthesis}
}

@INPROCEEDINGS{ferrandi2021bambu,

	author = {Ferrandi, Fabrizio and Castellana, Vito Giovanni

			and Curzel, Serena and Fezzardi, Pietro and Fiorito, Michele

			and Lattuada, Marco and Minutoli, Marco and Pilato, Christian

			and Tumeo, Antonino},

	booktitle = {2021 58th ACM/IEEE Design Automation Conference (DAC)},

	title = {Invited: Bambu: an Open-Source Research Framework for the

			High-Level Synthesis of Complex Applications},

	year = {2021},

	pages = {1327-1330},

	publisher = {{IEEE}},

}

@phdthesis{tuprints9272,
	title = {Advances in ILP-based Modulo Scheduling for High-Level Synthesis},
	year = {2019},
	language = {en},
	author = {Julian Oppermann},
	school = {Technische Universit{\"a}t},
	address = {Darmstadt},
	url = {http://tuprints.ulb.tu-darmstadt.de/9272/},
	abstract = {In today's heterogenous computing world, field-programmable gate arrays (FPGA) represent the energy-efficient alternative to generic processor cores and graphics accelerators. However, due to their radically different computing model, automatic design methods, such as high-level synthesis (HLS), are needed to harness their full power. HLS raises the abstraction level to behavioural descriptions of algorithms, thus freeing designers from dealing with tedious low-level concerns, and enabling a rapid exploration of different microarchitectures for the same input specification. In an HLS tool, scheduling is the most influential step for the performance of the generated accelerator. Specifically, modulo schedulers enable a pipelined execution, which is a key technique to speed up the computation by extracting more parallelism from the input description. In this thesis, we make a case for the use of integer linear programming (ILP) as a framework for modulo scheduling approaches. First, we argue that ILP-based modulo schedulers are practically usable in the HLS context. Secondly, we show that the ILP framework enables a novel approach for the automatic design of FPGA accelerators. We substantiate the first claim by proposing a new, flexible ILP formulation for the modulo scheduling problem, and evaluate it experimentally with a diverse set of realistic test instances. While solving an ILP may incur an exponential runtime in the worst case, we observe that simple countermeasures, such as setting a time limit, help to contain the practical impact of outlier instances. Furthermore, we present an algorithm to compress problems before the actual scheduling. An HLS-generated microarchitecture is comprised of operators, i.e. single-purpose functional units such as a floating-point multiplier. Usually, the allocation of operators is determined before scheduling, even though both problems are interdependent. To that end, we investigate an extension of the modulo scheduling problem that combines both concerns in a single model. Based on the extension, we present a novel multi-loop scheduling approach capable of finding the fastest microarchitecture that still fits on a given FPGA device - an optimisation problem that current commercial HLS tools cannot solve. This proves our second claim.}
}

@INPROCEEDINGS{1688836,  author = {Cong, J. and Zhiru Zhang},  booktitle = {2006 43rd ACM/IEEE Design Automation Conference},   title = {An efficient and versatile scheduling algorithm based on SDC formulation},   year = {2006},  volume = {},  number = {},  pages = {433-438},  doi = {10.1145/1146909.1147025} }

@INPROCEEDINGS{6546003,  author={Soni, Ritesh Kumar and Steiner, Neil and French, Matthew},  booktitle={2013 IEEE 21st Annual International Symposium on Field-Programmable Custom Computing Machines},   title={Open-Source Bitstream Generation},   year={2013},  volume={},  number={},  pages={105-112},  doi={10.1109/FCCM.2013.45}}

@inproceedings{wolf2013yosys,
  title={Yosys-a free Verilog synthesis suite},
  author={Wolf, Clifford and Glaser, Johann and Kepler, Johannes},
  booktitle={Proceedings of the 21st Austrian Workshop on Microelectronics (Austrochip)},
  year={2013}
}

@inproceedings{10.1145/2688500.2688521,
author = {Ashari, Arash and Tatikonda, Shirish and Boehm, Matthias and Reinwald, Berthold and Campbell, Keith and Keenleyside, John and Sadayappan, P.},
title = {On Optimizing Machine Learning Workloads via Kernel Fusion},
year = {2015},
isbn = {9781450332057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2688500.2688521},
doi = {10.1145/2688500.2688521},
abstract = { Exploitation of parallel architectures has become critical to scalable machine learning (ML). Since a wide range of ML algorithms employ linear algebraic operators, GPUs with BLAS libraries are a natural choice for such an exploitation. Two approaches are commonly pursued: (i) developing specific GPU accelerated implementations of complete ML algorithms; and (ii) developing GPU kernels for primitive linear algebraic operators like matrix-vector multiplication, which are then used in developing ML algorithms. This paper extends the latter approach by developing fused kernels for a combination of primitive operators that are commonly found in popular ML algorithms. We identify the generic pattern of computation (alpha * X^T (v * (X * y)) + beta * z) and its various instantiations. We develop a fused kernel to optimize this computation on GPUs -- with specialized techniques to handle both sparse and dense matrices. This approach not only reduces the cost of data loads due to improved temporal locality but also enables other optimizations like coarsening and hierarchical aggregation of partial results. We also present an analytical model that considers input data characteristics and available GPU resources to estimate near-optimal settings for kernel launch parameters. The proposed approach provides speedups ranging from 2 to 67 for different instances of the generic pattern compared to launching multiple operator-level kernels using GPU accelerated libraries. We conclude by demonstrating the effectiveness of the approach in improving end-to-end performance on an entire ML algorithm. },
booktitle = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {173–182},
numpages = {10},
keywords = {Sparse, Fused Kernel, GPU, Dense, Machine Learning},
location = {San Francisco, CA, USA},
series = {PPoPP 2015}
}

@article{10.1145/2858788.2688521,
author = {Ashari, Arash and Tatikonda, Shirish and Boehm, Matthias and Reinwald, Berthold and Campbell, Keith and Keenleyside, John and Sadayappan, P.},
title = {On Optimizing Machine Learning Workloads via Kernel Fusion},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858788.2688521},
doi = {10.1145/2858788.2688521},
abstract = { Exploitation of parallel architectures has become critical to scalable machine learning (ML). Since a wide range of ML algorithms employ linear algebraic operators, GPUs with BLAS libraries are a natural choice for such an exploitation. Two approaches are commonly pursued: (i) developing specific GPU accelerated implementations of complete ML algorithms; and (ii) developing GPU kernels for primitive linear algebraic operators like matrix-vector multiplication, which are then used in developing ML algorithms. This paper extends the latter approach by developing fused kernels for a combination of primitive operators that are commonly found in popular ML algorithms. We identify the generic pattern of computation (alpha * X^T (v * (X * y)) + beta * z) and its various instantiations. We develop a fused kernel to optimize this computation on GPUs -- with specialized techniques to handle both sparse and dense matrices. This approach not only reduces the cost of data loads due to improved temporal locality but also enables other optimizations like coarsening and hierarchical aggregation of partial results. We also present an analytical model that considers input data characteristics and available GPU resources to estimate near-optimal settings for kernel launch parameters. The proposed approach provides speedups ranging from 2 to 67 for different instances of the generic pattern compared to launching multiple operator-level kernels using GPU accelerated libraries. We conclude by demonstrating the effectiveness of the approach in improving end-to-end performance on an entire ML algorithm. },
journal = {SIGPLAN Not.},
month = {jan},
pages = {173–182},
numpages = {10},
keywords = {Dense, GPU, Machine Learning, Sparse, Fused Kernel}
}


@article{osti_1574050,
title = {Hierarchical Roofline analysis for GPUs: Accelerating performance optimization for the NERSC‐9 Perlmutter system},
author = {Yang, Charlene and Kurth, Thorsten and Williams, Samuel},
abstractNote = {},
doi = {10.1002/cpe.5547},
journal = {Concurrency and Computation. Practice and Experience},
number = 20,
volume = 32,
place = {United Kingdom},
year = {2019},
month = {11}
}

@inproceedings{10.1145/3295500.3356173,
author = {Ben-Nun, Tal and de Fine Licht, Johannes and Ziogas, Alexandros N. and Schneider, Timo and Hoefler, Torsten},
title = {Stateful Dataflow Multigraphs: A Data-Centric Model for Performance Portability on Heterogeneous Architectures},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356173},
doi = {10.1145/3295500.3356173},
abstract = {The ubiquity of accelerators in high-performance computing has driven programming complexity beyond the skill-set of the average domain scientist. To maintain performance portability in the future, it is imperative to decouple architecture-specific programming paradigms from the underlying scientific computations. We present the Stateful DataFlow multiGraph (SDFG), a data-centric intermediate representation that enables separating program definition from its optimization. By combining fine-grained data dependencies with high-level control-flow, SDFGs are both expressive and amenable to program transformations, such as tiling and double-buffering. These transformations are applied to the SDFG in an interactive process, using extensible pattern matching, graph rewriting, and a graphical user interface. We demonstrate SDFGs on CPUs, GPUs, and FPGAs over various motifs --- from fundamental computational kernels to graph analytics. We show that SDFGs deliver competitive performance, allowing domain scientists to develop applications naturally and port them to approach peak hardware performance without modifying the original scientific code.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {81},
numpages = {14},
location = {Denver, Colorado},
series = {SC '19}
}

@book{thomas1971catalogue,
  title={A catalogue of optimizing transformations},
  author={Thomas J. Watson IBM Research Center. Research Division and Allen, FE and Cocke, J},
  year={1971}
}


@book{sanders2019sequential,
  title={Sequential and Parallel Algorithms and Data Structures},
  author={Sanders, Peter and Mehlhorn, Kurt and Dietzfelbinger, Martin and Dementiev, Roman},
  year={2019},
  publisher={Springer},
  chapter={9},
  section={9.4},
}

  @misc{ enwiki:1081681080,
    author = "{Wikipedia contributors}",
    title = "Fast inverse square root --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2022",
    howpublished = "\url{https://en.wikipedia.org/wiki/Fast_inverse_square_root}",
    note = "[Online; accessed 3-May-2022]"
  }


@inproceedings{10.1007/978-0-387-72258-0_14,
	address = {Boston, MA},
	author = {Middendorf, Lars and M{\"u}hlbauer, Felix and Umlauf, Georg and Bobda, Christophe},
	booktitle = {Embedded System Design: Topics, Techniques and Trends},
	editor = {Rettberg, Achim and Zanella, Mauro C. and D{\"o}mer, Rainer and Gerstlauer, Andreas and Rammig, Franz J.},
	isbn = {978-0-387-72258-0},
	pages = {155--164},
	publisher = {Springer US},
	title = {Embedded Vertex Shader in FPGA},
	year = {2007}}

@INPROCEEDINGS{8877424,  author={de Dinechin, Florent},  booktitle={2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)},   title={Reflections on 10 Years of FloPoCo},   year={2019},  volume={},  number={},  pages={187-189},  doi={10.1109/ARITH.2019.00042}}


@article{ye2021scalehls,
  title={ScaleHLS: A New Scalable High-Level Synthesis Framework on Multi-Level Intermediate Representation},
  author={Ye, Hanchen and Hao, Cong and Cheng, Jianyi and Jeong, Hyunmin and Huang, Jack and Neuendorffer, Stephen and Chen, Deming},
  journal={arXiv preprint arXiv:2107.11673},
  year={2021}
}

@INPROCEEDINGS{9516615,  author={Zhang, Jeff Jun and Bohm Agostini, Nicolas and Song, Shihao and Tan, Cheng and Limaye, Ankur and Amatya, Vinay and Manzano, Joseph and Minutoli, Marco and Castellana, Vito Giovanni and Tumeo, Antonino and Wei, Gu-Yeon and Brooks, David},  booktitle={2021 IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP)},   title={Towards Automatic and Agile AI/ML Accelerator Design with End-to-End Synthesis},   year={2021},  volume={},  number={},  pages={218-225},  doi={10.1109/ASAP52443.2021.00040}}

@inproceedings{nikhil2004bluespec,
  title={Bluespec System Verilog: efficient, correct RTL from high level specifications},
  author={Nikhil, Rishiyur},
  booktitle={Proceedings. Second ACM and IEEE International Conference on Formal Methods and Models for Co-Design, 2004. MEMOCODE'04.},
  pages={69--70},
  year={2004},
  organization={IEEE}
}


@article{reconfigfpga,
	abstract = {Reconfigurable computing is a potential paradigm which has been effectively performing mostly in the developments of devices likely Field Programmable Gate Arrays (FPGAs). This paper illustrates the reconfigurable architecture of FPGA and its types. Most widely used high-speed computation fabrics utilized in reconfigurable computing are FPGAs. This paper demonstrates the architectures used in reconfigurable computing and shows the various advantages of using reconfigurable computing design over conventional Application-Specific Integrated Circuits for achieving high level of performance for a desired application. The survey deals with the architecture of FPGAs and their types in detail. This paper also explains the highlights and challenges of fine-grained and coarse-grained architectures. FPGAs have supported partial reconfiguration over the few years. This survey also includes the partial reconfiguration techniques and the various applications of reconfigurability.},
	author = {Babu, Praveenkumar and Parthasarathy, Eswaran},
	date = {2021/02/01},
	date-added = {2022-05-03 17:40:19 -0500},
	date-modified = {2022-05-03 17:40:19 -0500},
	doi = {10.1007/s40031-020-00508-y},
	id = {Babu2021},
	isbn = {2250-2114},
	journal = {Journal of The Institution of Engineers (India): Series B},
	number = {1},
	pages = {143--156},
	title = {Reconfigurable FPGA Architectures: A Survey and Applications},
	url = {https://doi.org/10.1007/s40031-020-00508-y},
	volume = {102},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s40031-020-00508-y}}

@inproceedings{10.1145/3211346.3211348,
author = {Roesch, Jared and Lyubomirsky, Steven and Weber, Logan and Pollock, Josh and Kirisame, Marisa and Chen, Tianqi and Tatlock, Zachary},
title = {Relay: A New IR for Machine Learning Frameworks},
year = {2018},
isbn = {9781450358347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211346.3211348},
doi = {10.1145/3211346.3211348},
abstract = {Machine learning powers diverse services in industry including search, translation, recommendation systems, and security. The scale and importance of these models require that they be efficient, expressive, and portable across an array of heterogeneous hardware devices. These constraints are often at odds; in order to better accommodate them we propose a new high-level intermediate representation (IR) called Relay. Relay is being designed as a purely-functional, statically-typed language with the goal of balancing efficient compilation, expressiveness, and portability. We discuss the goals of Relay and highlight its important design constraints. Our prototype is part of the open source NNVM compiler framework, which powers Amazon's deep learning framework MxNet.},
booktitle = {Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {58–68},
numpages = {11},
keywords = {intermediate representation, compilers, machine learning, differentiable programming},
location = {Philadelphia, PA, USA},
series = {MAPL 2018}
}

@article{vtr,
author = {Murray, Kevin and Petelin, Oleg and Zhong, Sheng and Wang, Jia and Eldafrawy, Mohamed and Legault, Jean-Philippe and Sha, Eugene and Graham, Aaron and Wu, Jean and Walker, Matthew and Zeng, Hanqing and Patros, Panos and Luu, Jason and Kent, Kenneth and Betz, Vaughn},
year = {2020},
month = {05},
pages = {1-55},
title = {VTR 8: High-performance CAD and Customizable FPGA Architecture Modelling},
volume = {13},
journal = {ACM Transactions on Reconfigurable Technology and Systems},
doi = {10.1145/3388617}
}

@article{10.2307/2322281,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2322281},
 author = {H. P. Williams},
 journal = {The American Mathematical Monthly},
 number = {9},
 pages = {681--695},
 publisher = {Mathematical Association of America},
 title = {Fourier's Method of Linear Programming and Its Dual},
 urldate = {2022-06-10},
 volume = {93},
 year = {1986}
}



@article{Sharma:rw5009,
	abstract = {A data-analysis methodology is presented for the characterization of three-dimensional microstructures of polycrystalline materials from data acquired using three-dimensional X-ray diffraction (3DXRD). The method is developed for 3DXRD microscopy using a far-field detector and yields information about the centre-of-mass position, crystallographic orientation, volume and strain state for thousands of grains. This first part deals with pre-processing of the diffraction data for input into the algorithms presented in the second part [Sharma, Huizenga & Offerman (2012). {\it J. Appl. Cryst.} {\bf 45}, 705{--}718] for determination of the grain characteristics. An algorithm is presented for accurate identification of overlapping diffraction peaks from X-ray diffraction images, which has been an issue limiting the accuracy of experiments of this type. The algorithm works in two stages, namely the identification of overlapping peaks using a seeded watershed algorithm, and then the fitting of the peaks with a pseudo-Voigt shape function to yield an accurate centre-of-mass position and integrated intensity for the peaks. Regions consisting of up to six overlapping peaks can be successfully fitted. Two simulations and an experiment are used to verify the results of the algorithms. An example of the processing of diffraction images acquired in a 3DXRD experiment with a sample consisting of more than 1600 grains is shown. Furthermore, a procedure for the determination of the parameters of the experimental setup (global parameters) without the need for a calibration sample is presented and validated using simulations. This is immensely beneficial for simplifying experiments and the subsequent data analysis.},
	author = {Sharma, Hemant and Huizenga, Richard M. and Offerman, S. Erik},
	doi = {10.1107/S0021889812025563},
	journal = {Journal of Applied Crystallography},
	keywords = {three-dimensional X-ray diffraction, peak fitting, peak overlap, experimental parameters, synchrotron radiation, diffraction, microscopy},
	month = {Aug},
	number = {4},
	pages = {693--704},
	title = {{A fast methodology to determine the characteristics of thousands of grains using three-dimensional X-ray diffraction. I. Overlapping diffraction peaks and parameters of the experimental setup}},
	url = {https://doi.org/10.1107/S0021889812025563},
	volume = {45},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1107/S0021889812025563}}


@article{Liu:fs5198,
	author = {Liu, Zhengchun and Sharma, Hemant and Park, Jun-Sang and Kenesei, Peter and Miceli, Antonino and Almer, Jonathan and Kettimuthu, Rajkumar and Foster, Ian},
	journal = {IUCrJ},
	month = {Jan},
	number = {1},
	pages = {104--113},
	title = {{{\it BraggNN}: fast X-ray Bragg peak analysis using deep learning}},
	volume = {9},
	year = {2022}}


@INPROCEEDINGS{8237584,  author={He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},   title={Mask R-CNN},   year={2017},  volume={},  number={},  pages={2980-2988},  doi={10.1109/ICCV.2017.322}}